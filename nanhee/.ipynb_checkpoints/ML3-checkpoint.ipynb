{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exclusive-religious",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\enssel\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from xgboost) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from xgboost) (1.6.2)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\enssel\\anaconda3\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: wheel in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from lightgbm) (0.36.2)\n",
      "Requirement already satisfied: scikit-learn!=0.22.0 in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from lightgbm) (0.24.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from lightgbm) (1.19.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from lightgbm) (1.6.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from scikit-learn!=0.22.0->lightgbm) (2.1.0)\n",
      "Requirement already satisfied: imblearn in c:\\users\\enssel\\anaconda3\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: imbalanced-learn in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from imblearn) (0.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.19.2)\n",
      "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.6.2)\n",
      "Requirement already satisfied: scikit-learn>=0.24 in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (0.24.1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from imbalanced-learn->imblearn) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\enssel\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn->imblearn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "!pip install xgboost\n",
    "!pip install lightgbm\n",
    "!pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "analyzed-entity",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('Survival_time_event.csv', index_col=0)\n",
    "df2=pd.read_csv('Treatment.csv', index_col=0)\n",
    "df3=pd.read_csv('Clinical_Variables.csv', index_col=0)\n",
    "df4=pd.read_csv('Genetic_alterations.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "listed-satellite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Treatment : 488\n",
      "No Treatment : 512\n",
      "Total : 1000\n",
      "\n",
      "Survive : 109\n",
      "No Survive : 891\n",
      "Total : 1000\n",
      "\n",
      "Treatment & Survive : 48\n",
      "Treatment & No Survive : 440\n",
      "No Treatment & Survive : 61\n",
      "No Treatment & No Survive : 451\n",
      "Total : 1000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_path = './'\n",
    "\n",
    "clinic_table = df3.to_numpy()[:,:]\n",
    "genetic_table = df4.to_numpy()[:,:]\n",
    "survival_table = df1.to_numpy()[:,:]\n",
    "treatment_table = df2.to_numpy()[:,:]\n",
    "\n",
    "treatment = 1\n",
    "no_treatmet = 0\n",
    "treatment_index = np.where(treatment==treatment_table)[0] # 치료를 받은 사람들의 Index (총 488명)\n",
    "no_treatment_index = np.where(no_treatmet==treatment_table)[0] # 치료를 받지 않은 사람들의 Index (총 512명)\n",
    "print('Treatment :', treatment_index.shape[0])\n",
    "print('No Treatment :', no_treatment_index.shape[0])\n",
    "print('Total :', treatment_index.shape[0] + no_treatment_index.shape[0])\n",
    "print()\n",
    "\n",
    "survive = 0\n",
    "no_survive = 1\n",
    "survive_index = np.where(survive==survival_table[:,1])[0] # 생존한 사람들의 Index (총 109명)\n",
    "no_survive_index = np.where(no_survive==survival_table[:,1])[0] # 생존하지 못 한 사람들의 Index (총 891명)\n",
    "print('Survive :', survive_index.shape[0])\n",
    "print('No Survive :', no_survive_index.shape[0])\n",
    "print('Total :', survive_index.shape[0] + no_survive_index.shape[0])\n",
    "print()\n",
    "\n",
    "treatment_survive_index = np.intersect1d(treatment_index, survive_index) # 치료 O, 생존 O (총 48명)\n",
    "treatment_no_survive_index = np.intersect1d(treatment_index, no_survive_index) # 치료 O, 생존 X (총 440명)\n",
    "no_treatment_survive_index = np.intersect1d(no_treatment_index, survive_index) # 치료 X, 생존 O (총 61명)\n",
    "no_treatment_no_survive_index = np.intersect1d(no_treatment_index, no_survive_index) # 치료 X, 생존 X (총 451명)\n",
    "print('Treatment & Survive :', treatment_survive_index.shape[0])\n",
    "print('Treatment & No Survive :', treatment_no_survive_index.shape[0])\n",
    "print('No Treatment & Survive :', no_treatment_survive_index.shape[0])\n",
    "print('No Treatment & No Survive :', no_treatment_no_survive_index.shape[0])\n",
    "print('Total :', treatment_survive_index.shape[0] + treatment_no_survive_index.shape[0] + no_treatment_survive_index.shape[0] + no_treatment_no_survive_index.shape[0])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "continental-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TandS = ['newlabel']\n",
    "# df5 = pd.DataFrame(columns=TandS, index=range(0,1000))\n",
    "# print(df5)\n",
    "# df5.iloc[treatment_survive_index,0] = [0] * treatment_survive_index.shape[0]\n",
    "# df5.iloc[treatment_no_survive_index,0] = [1] * treatment_no_survive_index.shape[0]\n",
    "# df5.iloc[no_treatment_survive_index,0] = [2] * no_treatment_survive_index.shape[0]\n",
    "# df5.iloc[no_treatment_no_survive_index,0] = [3] * no_treatment_no_survive_index.shape[0]\n",
    "\n",
    "# print(df5)\n",
    "# df5.to_csv('newLabel.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "northern-training",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5=pd.read_csv('newLabel.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aware-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df5,df1,df2,df3,df4], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "municipal-percentage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['newlabel' 'time' 'Var1' 'Var2' 'Var3' 'Var4' 'Var5' 'Var6' 'Var7' 'Var8'\n",
      " 'Var9' 'Var10' 'G1' 'G2' 'G3' 'G4' 'G5' 'G6' 'G7' 'G8' 'G9' 'G10' 'G11'\n",
      " 'G12' 'G13' 'G14' 'G15' 'G16' 'G17' 'G18' 'G19' 'G20' 'G21' 'G22' 'G23'\n",
      " 'G24' 'G25' 'G26' 'G27' 'G28' 'G29' 'G30' 'G31' 'G32' 'G33' 'G34' 'G35'\n",
      " 'G36' 'G37' 'G38' 'G39' 'G40' 'G41' 'G42' 'G43' 'G44' 'G45' 'G46' 'G47'\n",
      " 'G48' 'G49' 'G50' 'G51' 'G52' 'G53' 'G54' 'G55' 'G56' 'G57' 'G58' 'G59'\n",
      " 'G60' 'G61' 'G62' 'G63' 'G64' 'G65' 'G66' 'G67' 'G68' 'G69' 'G70' 'G71'\n",
      " 'G72' 'G73' 'G74' 'G75' 'G76' 'G77' 'G78' 'G79' 'G80' 'G81' 'G82' 'G83'\n",
      " 'G84' 'G85' 'G86' 'G87' 'G88' 'G89' 'G90' 'G91' 'G92' 'G93' 'G94' 'G95'\n",
      " 'G96' 'G97' 'G98' 'G99' 'G100' 'G101' 'G102' 'G103' 'G104' 'G105' 'G106'\n",
      " 'G107' 'G108' 'G109' 'G110' 'G111' 'G112' 'G113' 'G114' 'G115' 'G116'\n",
      " 'G117' 'G118' 'G119' 'G120' 'G121' 'G122' 'G123' 'G124' 'G125' 'G126'\n",
      " 'G127' 'G128' 'G129' 'G130' 'G131' 'G132' 'G133' 'G134' 'G135' 'G136'\n",
      " 'G137' 'G138' 'G139' 'G140' 'G141' 'G142' 'G143' 'G144' 'G145' 'G146'\n",
      " 'G147' 'G148' 'G149' 'G150' 'G151' 'G152' 'G153' 'G154' 'G155' 'G156'\n",
      " 'G157' 'G158' 'G159' 'G160' 'G161' 'G162' 'G163' 'G164' 'G165' 'G166'\n",
      " 'G167' 'G168' 'G169' 'G170' 'G171' 'G172' 'G173' 'G174' 'G175' 'G176'\n",
      " 'G177' 'G178' 'G179' 'G180' 'G181' 'G182' 'G183' 'G184' 'G185' 'G186'\n",
      " 'G187' 'G188' 'G189' 'G190' 'G191' 'G192' 'G193' 'G194' 'G195' 'G196'\n",
      " 'G197' 'G198' 'G199' 'G200' 'G201' 'G202' 'G203' 'G204' 'G205' 'G206'\n",
      " 'G207' 'G208' 'G209' 'G210' 'G211' 'G212' 'G213' 'G214' 'G215' 'G216'\n",
      " 'G217' 'G218' 'G219' 'G220' 'G221' 'G222' 'G223' 'G224' 'G225' 'G226'\n",
      " 'G227' 'G228' 'G229' 'G230' 'G231' 'G232' 'G233' 'G234' 'G235' 'G236'\n",
      " 'G237' 'G238' 'G239' 'G240' 'G241' 'G242' 'G243' 'G244' 'G245' 'G246'\n",
      " 'G247' 'G248' 'G249' 'G250' 'G251' 'G252' 'G253' 'G254' 'G255' 'G256'\n",
      " 'G257' 'G258' 'G259' 'G260' 'G261' 'G262' 'G263' 'G264' 'G265' 'G266'\n",
      " 'G267' 'G268' 'G269' 'G270' 'G271' 'G272' 'G273' 'G274' 'G275' 'G276'\n",
      " 'G277' 'G278' 'G279' 'G280' 'G281' 'G282' 'G283' 'G284' 'G285' 'G286'\n",
      " 'G287' 'G288' 'G289' 'G290' 'G291' 'G292' 'G293' 'G294' 'G295' 'G296'\n",
      " 'G297' 'G298' 'G299' 'G300']\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Analyze by describing data\n",
    "'''\n",
    "'''\n",
    "label class\n",
    "\n",
    "치료o + 사망x -> class 1\n",
    "나머지 경우 -> class 0\n",
    "\n",
    "'''\n",
    "\n",
    "df = df.drop(['event', 'Treatment'], axis=1)\n",
    "\n",
    "print(df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "baking-nevada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   newlabel       time  Var1  Var2  Var3  Var4  Var5  Var6  Var7  Var8  ...  \\\n",
      "0         3  57.448331     5     1     1     4     6     5     2     1  ...   \n",
      "1         3  27.004439     3     1     3     9     1     1     2     4  ...   \n",
      "2         1  43.770511     2     5     3     4     3     3     3     2  ...   \n",
      "3         1  32.281018     2     7     2     3     5     0     1     4  ...   \n",
      "4         2  44.559284     1     3     0     0     2     2     6     3  ...   \n",
      "\n",
      "   G291  G292  G293  G294  G295  G296  G297  G298  G299  G300  \n",
      "0     0     0     0     0     0     0     0     0     0     0  \n",
      "1     0     0     0     0     0     0     0     0     0     0  \n",
      "2     0     0     0     0     0     0     0     0     0     0  \n",
      "3     0     1     0     1     0     0     0     0     0     0  \n",
      "4     0     0     0     0     0     0     0     0     0     1  \n",
      "\n",
      "[5 rows x 312 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "specified-bonus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     newlabel       time  Var1  Var2  Var3  Var4  Var5  Var6  Var7  Var8  ...  \\\n",
      "995         2  19.289036     3     5     3     7     0     2     1     4  ...   \n",
      "996         1  66.591235     4     2     1     2     2     2     2     1  ...   \n",
      "997         0  62.986021     4     3     4     9     3     6     6     4  ...   \n",
      "998         3  32.736220     4     1     4     5     6     3     1     2  ...   \n",
      "999         3  36.714493     3     2     1     2     4     3     1     2  ...   \n",
      "\n",
      "     G291  G292  G293  G294  G295  G296  G297  G298  G299  G300  \n",
      "995     0     0     0     0     0     0     1     0     0     0  \n",
      "996     0     0     1     0     1     0     0     0     1     0  \n",
      "997     0     0     0     0     1     0     0     0     0     0  \n",
      "998     0     0     0     0     0     0     0     1     1     0  \n",
      "999     0     0     0     0     0     0     0     0     0     0  \n",
      "\n",
      "[5 rows x 312 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "regular-aggregate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vocal-theme",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Train & Test Split\n",
    "'''\n",
    "random_index = np.arange(1000)\n",
    "# np.random.shuffle(random_index)\n",
    "# np.random.shuffle(random_index)\n",
    "\n",
    "train_df, test_df = df.iloc[random_index[:800],:], df.iloc[random_index[800:1000],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "effective-recall",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 312)\n",
      "   newlabel       time  Var1  Var2  Var3  Var4  Var5  Var6  Var7  Var8  ...  \\\n",
      "0         3  57.448331     5     1     1     4     6     5     2     1  ...   \n",
      "1         3  27.004439     3     1     3     9     1     1     2     4  ...   \n",
      "2         1  43.770511     2     5     3     4     3     3     3     2  ...   \n",
      "3         1  32.281018     2     7     2     3     5     0     1     4  ...   \n",
      "4         2  44.559284     1     3     0     0     2     2     6     3  ...   \n",
      "\n",
      "   G291  G292  G293  G294  G295  G296  G297  G298  G299  G300  \n",
      "0     0     0     0     0     0     0     0     0     0     0  \n",
      "1     0     0     0     0     0     0     0     0     0     0  \n",
      "2     0     0     0     0     0     0     0     0     0     0  \n",
      "3     0     1     0     1     0     0     0     0     0     0  \n",
      "4     0     0     0     0     0     0     0     0     0     1  \n",
      "\n",
      "[5 rows x 312 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape)\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lonely-inquiry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         newlabel        time        Var1        Var2        Var3        Var4  \\\n",
      "count  800.000000  800.000000  800.000000  800.000000  800.000000  800.000000   \n",
      "mean     1.906250   51.286198    2.918750    3.326250    3.022500    3.155000   \n",
      "std      1.032615   20.840875    1.878579    1.947962    1.907627    1.943115   \n",
      "min      0.000000    7.070708    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   37.385295    2.000000    2.000000    2.000000    2.000000   \n",
      "50%      2.000000   47.141058    3.000000    3.000000    3.000000    3.000000   \n",
      "75%      3.000000   60.610372    4.000000    4.000000    4.000000    4.000000   \n",
      "max      3.000000  171.994623    9.000000   11.000000   11.000000   12.000000   \n",
      "\n",
      "             Var5        Var6        Var7        Var8  ...       G291  \\\n",
      "count  800.000000  800.000000  800.000000  800.000000  ...  800.00000   \n",
      "mean     3.470000    3.265000    2.346250    2.387500  ...    0.10875   \n",
      "std      1.873763    2.028526    1.795938    1.838905  ...    0.31152   \n",
      "min      0.000000    0.000000    0.000000    0.000000  ...    0.00000   \n",
      "25%      2.000000    2.000000    1.000000    1.000000  ...    0.00000   \n",
      "50%      3.000000    3.000000    2.000000    2.000000  ...    0.00000   \n",
      "75%      5.000000    4.000000    3.000000    3.000000  ...    0.00000   \n",
      "max     10.000000   12.000000   10.000000    9.000000  ...    1.00000   \n",
      "\n",
      "             G292        G293        G294        G295        G296        G297  \\\n",
      "count  800.000000  800.000000  800.000000  800.000000  800.000000  800.000000   \n",
      "mean     0.090000    0.097500    0.111250    0.122500    0.110000    0.101250   \n",
      "std      0.286361    0.296823    0.314638    0.328068    0.313085    0.301848   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "            G298        G299        G300  \n",
      "count  800.00000  800.000000  800.000000  \n",
      "mean     0.08875    0.102500    0.096250  \n",
      "std      0.28456    0.303494    0.295118  \n",
      "min      0.00000    0.000000    0.000000  \n",
      "25%      0.00000    0.000000    0.000000  \n",
      "50%      0.00000    0.000000    0.000000  \n",
      "75%      0.00000    0.000000    0.000000  \n",
      "max      1.00000    1.000000    1.000000  \n",
      "\n",
      "[8 rows x 312 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "naked-ethernet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         newlabel        time        Var1       Var2        Var3        Var4  \\\n",
      "count  200.000000  200.000000  200.000000  200.00000  200.000000  200.000000   \n",
      "mean     1.950000   54.156377    3.125000    3.19500    2.915000    3.440000   \n",
      "std      1.059771   26.732912    1.848366    1.97903    1.943363    2.026557   \n",
      "min      0.000000   -7.945621    0.000000    0.00000    0.000000    0.000000   \n",
      "25%      1.000000   37.611163    2.000000    2.00000    2.000000    2.000000   \n",
      "50%      2.000000   46.796557    3.000000    3.00000    2.000000    3.000000   \n",
      "75%      3.000000   62.989101    4.000000    4.00000    4.000000    5.000000   \n",
      "max      3.000000  217.078908   10.000000   10.00000    9.000000    9.000000   \n",
      "\n",
      "            Var5        Var6        Var7        Var8  ...        G291  \\\n",
      "count  200.00000  200.000000  200.000000  200.000000  ...  200.000000   \n",
      "mean     3.56500    3.120000    2.415000    2.420000  ...    0.125000   \n",
      "std      2.06813    2.072941    1.913395    1.791549  ...    0.331549   \n",
      "min      0.00000    0.000000    0.000000    0.000000  ...    0.000000   \n",
      "25%      2.00000    2.000000    1.000000    1.000000  ...    0.000000   \n",
      "50%      3.00000    3.000000    2.000000    2.000000  ...    0.000000   \n",
      "75%      5.00000    4.000000    4.000000    3.000000  ...    0.000000   \n",
      "max     11.00000   10.000000    8.000000   10.000000  ...    1.000000   \n",
      "\n",
      "             G292        G293        G294        G295        G296        G297  \\\n",
      "count  200.000000  200.000000  200.000000  200.000000  200.000000  200.000000   \n",
      "mean     0.085000    0.120000    0.080000    0.130000    0.095000    0.145000   \n",
      "std      0.279582    0.325777    0.271974    0.337147    0.293951    0.352984   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "50%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "75%      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "max      1.000000    1.000000    1.000000    1.000000    1.000000    1.000000   \n",
      "\n",
      "             G298        G299        G300  \n",
      "count  200.000000  200.000000  200.000000  \n",
      "mean     0.095000    0.085000    0.100000  \n",
      "std      0.293951    0.279582    0.300753  \n",
      "min      0.000000    0.000000    0.000000  \n",
      "25%      0.000000    0.000000    0.000000  \n",
      "50%      0.000000    0.000000    0.000000  \n",
      "75%      0.000000    0.000000    0.000000  \n",
      "max      1.000000    1.000000    1.000000  \n",
      "\n",
      "[8 rows x 312 columns]\n"
     ]
    }
   ],
   "source": [
    "print(test_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "proud-bronze",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 310) (800,) (200, 310) (200,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Data & Ground Truth Split\n",
    "'''\n",
    "\n",
    "'''\n",
    "Time data drop\n",
    "'''\n",
    "\n",
    "X_train = train_df.drop(['time', 'newlabel'], axis=1)\n",
    "Y_train = train_df['newlabel']\n",
    "\n",
    "X_test = test_df.drop(['time', 'newlabel'], axis=1)\n",
    "Y_test = test_df['newlabel']\n",
    "\n",
    "print(X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "temporal-powder",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Save train & test results\n",
    "\n",
    "When doing train, Shuffle and split the data\n",
    "\n",
    "'''\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV, RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "test_method_list = ['LogisticRegression', 'Support Vector Machines', 'KNN or k-Nearest Neighbors',\n",
    "                    'Gaussian Naive Bayes', 'Perceptron', 'Linear SVC', \n",
    "                    'Stochastic Gradient Descent', 'Decision Tree', 'Random Forest', 'Ensemble']\n",
    "\n",
    "ensemble_models = [\n",
    "    ('lrcv', LogisticRegressionCV(max_iter = 5000)),\n",
    "    ('ada', AdaBoostClassifier()),\n",
    "    ('bc', BaggingClassifier()),\n",
    "    ('etc',ExtraTreesClassifier()),\n",
    "    ('gbc', GradientBoostingClassifier()),\n",
    "    ('rfc', RandomForestClassifier(n_estimators=20)),\n",
    "    ('knn', KNeighborsClassifier(n_neighbors = 4)),\n",
    "    ('svc', SVC(probability=True)),\n",
    "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss')),\n",
    "    ('lgbm', LGBMClassifier()),\n",
    "    ('dtc', DecisionTreeClassifier()),\n",
    "    ('gnb',GaussianNB()),\n",
    "]\n",
    "\n",
    "index = ['Basic','Time_outlier','Time_/365','Clinic_outlier','Clinic_0-4(2steps)','Clinic_0-1(resizing)']\n",
    "\n",
    "result_train_df = pd.DataFrame(columns=test_method_list)\n",
    "result_test_df = pd.DataFrame(columns=test_method_list)\n",
    "result_acc_test_df = pd.DataFrame(columns=test_method_list)\n",
    "result_pre_test_df = pd.DataFrame(columns=test_method_list)\n",
    "result_rec_test_df = pd.DataFrame(columns=test_method_list)\n",
    "result_f1score_test_df = pd.DataFrame(columns=test_method_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "first-product",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Model, predict and solve\n",
    "\n",
    "\n",
    "- Methods\n",
    "Logistic Regression\n",
    "KNN or k-Nearest Neighbors\n",
    "Support Vector Machines\n",
    "Naive Bayes classifier\n",
    "Decision Tree\n",
    "Random Forrest\n",
    "Perceptron\n",
    "Artificial neural network\n",
    "RVM or Relevance Vector Machine\n",
    "Ensemble model\n",
    "'''\n",
    "\n",
    "\n",
    "def get_clf_eval(y_test, pred):\n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, average='weighted')\n",
    "    recall = recall_score(y_test, pred, average='weighted')\n",
    "    #     print('confusion matrix')\n",
    "    #     print(confusion)\n",
    "    print('accuracy:{}\\nprecision:{}\\nrecall:{}'.format(accuracy, precision, recall))\n",
    "    \n",
    "    return accuracy, precision, recall\n",
    "\n",
    "\n",
    "def training():\n",
    "    result_train = []\n",
    "    result_test = []\n",
    "    result_acc = []\n",
    "    result_pre = []\n",
    "    result_rec = []\n",
    "    result_f1 = []\n",
    "    for input_method in test_method_list:\n",
    "        print('*** Method: ', input_method)\n",
    "\n",
    "        if input_method == 'LogisticRegression':\n",
    "            model = LogisticRegression(max_iter = 10000)\n",
    "\n",
    "        elif input_method == 'Support Vector Machines':\n",
    "            model = SVC()\n",
    "\n",
    "        elif input_method == 'KNN or k-Nearest Neighbors':\n",
    "            model = KNeighborsClassifier(n_neighbors = 4)\n",
    "\n",
    "        elif input_method == 'Gaussian Naive Bayes':\n",
    "            model = GaussianNB()\n",
    "\n",
    "        elif input_method == 'Perceptron':\n",
    "            model = Perceptron()\n",
    "\n",
    "        elif input_method == 'Linear SVC':\n",
    "            model = LinearSVC(max_iter=1000000)\n",
    "\n",
    "        elif input_method == 'Stochastic Gradient Descent':\n",
    "            model = SGDClassifier()\n",
    "\n",
    "        elif input_method == 'Decision Tree':\n",
    "            model = DecisionTreeClassifier()\n",
    "\n",
    "        elif input_method == 'Random Forest':\n",
    "            model = RandomForestClassifier(n_estimators=20)\n",
    "            \n",
    "        elif input_method == 'Ensemble':\n",
    "            model  = VotingClassifier(ensemble_models, voting='soft') #, weights=[0]*len(ensemble_models)\n",
    "    \n",
    "        # Train\n",
    "        model.fit(X_train, Y_train)\n",
    "        acc_log = round(model.score(X_train, Y_train) * 100, 2)\n",
    "        print('trained-acc: ', acc_log)\n",
    "        result_train.append(acc_log)\n",
    "\n",
    "\n",
    "        # Test\n",
    "#         acc_log = round(model.score(X_test, Y_test) * 100, 2)\n",
    "        print('test- ')\n",
    "#         result_test.append(acc_log)\n",
    "        \n",
    "        # f1 score\n",
    "        Y_pred = model.predict(X_test)\n",
    "        acc, pre, rec = get_clf_eval(Y_test, Y_pred)\n",
    "        result_acc.append(acc)\n",
    "        result_pre.append(pre)\n",
    "        result_rec.append(rec)\n",
    "        f1 = f1_score(Y_test, Y_pred, average='weighted')\n",
    "        print('f1 score:{}'.format(f1))\n",
    "        result_f1.append(f1)\n",
    "        \n",
    "        print()\n",
    "        print()\n",
    "\n",
    "    result_train_df.loc[result_train_df.shape[0]] = result_train\n",
    "#     result_test_df.loc[result_test_df.shape[0]] = result_test\n",
    "    result_acc_test_df.loc[result_acc_test_df.shape[0]] = result_acc\n",
    "    result_pre_test_df.loc[result_pre_test_df.shape[0]] = result_pre\n",
    "    result_rec_test_df.loc[result_rec_test_df.shape[0]] = result_rec\n",
    "    result_f1score_test_df.loc[result_f1score_test_df.shape[0]] = result_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "centered-month",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Method:  LogisticRegression\n",
      "trained-acc:  85.62\n",
      "test- \n",
      "accuracy:0.435\n",
      "precision:0.43892307692307697\n",
      "recall:0.435\n",
      "f1 score:0.4355051091846158\n",
      "\n",
      "\n",
      "*** Method:  Support Vector Machines\n",
      "trained-acc:  70.0\n",
      "test- \n",
      "accuracy:0.485\n",
      "precision:0.44589538739825146\n",
      "recall:0.485\n",
      "f1 score:0.46459803921568626\n",
      "\n",
      "\n",
      "*** Method:  KNN or k-Nearest Neighbors\n",
      "trained-acc:  65.38\n",
      "test- \n",
      "accuracy:0.495\n",
      "precision:0.4900438596491228\n",
      "recall:0.495\n",
      "f1 score:0.4841293891812613\n",
      "\n",
      "\n",
      "*** Method:  Gaussian Naive Bayes\n",
      "trained-acc:  38.62\n",
      "test- \n",
      "accuracy:0.285\n",
      "precision:0.4842373579233588\n",
      "recall:0.285\n",
      "f1 score:0.3336797666152505\n",
      "\n",
      "\n",
      "*** Method:  Perceptron\n",
      "trained-acc:  64.88\n",
      "test- \n",
      "accuracy:0.45\n",
      "precision:0.42116005616005614\n",
      "recall:0.45\n",
      "f1 score:0.3982341601336589\n",
      "\n",
      "\n",
      "*** Method:  Linear SVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained-acc:  86.0\n",
      "test- \n",
      "accuracy:0.425\n",
      "precision:0.43592707454289736\n",
      "recall:0.425\n",
      "f1 score:0.42996592836749076\n",
      "\n",
      "\n",
      "*** Method:  Stochastic Gradient Descent\n",
      "trained-acc:  81.12\n",
      "test- \n",
      "accuracy:0.44\n",
      "precision:0.44318909538737133\n",
      "recall:0.44\n",
      "f1 score:0.43604805302402655\n",
      "\n",
      "\n",
      "*** Method:  Decision Tree\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.39\n",
      "precision:0.4161903147042865\n",
      "recall:0.39\n",
      "f1 score:0.39955799195777836\n",
      "\n",
      "\n",
      "*** Method:  Random Forest\n",
      "trained-acc:  99.88\n",
      "test- \n",
      "accuracy:0.42\n",
      "precision:0.38863028841322483\n",
      "recall:0.42\n",
      "f1 score:0.4021508410200759\n",
      "\n",
      "\n",
      "*** Method:  Ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.505\n",
      "precision:0.4656332533013206\n",
      "recall:0.505\n",
      "f1 score:0.4838510378510378\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Basic Training & Test\n",
    "\n",
    "'''\n",
    "\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "strange-norfolk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outlier of time: \n",
      "905   -7.945621\n",
      "Name: time, dtype: float64\n",
      "\n",
      "              time        event\n",
      "count  1000.000000  1000.000000\n",
      "mean     51.876125     0.891000\n",
      "std      22.122689     0.311795\n",
      "min       7.070708     0.000000\n",
      "25%      37.401307     1.000000\n",
      "50%      47.064712     1.000000\n",
      "75%      60.966476     1.000000\n",
      "max     217.078908     1.000000\n",
      "\n",
      "['newlabel' 'time' 'Var1' 'Var2' 'Var3' 'Var4' 'Var5' 'Var6' 'Var7' 'Var8'\n",
      " 'Var9' 'Var10' 'G1' 'G2' 'G3' 'G4' 'G5' 'G6' 'G7' 'G8' 'G9' 'G10' 'G11'\n",
      " 'G12' 'G13' 'G14' 'G15' 'G16' 'G17' 'G18' 'G19' 'G20' 'G21' 'G22' 'G23'\n",
      " 'G24' 'G25' 'G26' 'G27' 'G28' 'G29' 'G30' 'G31' 'G32' 'G33' 'G34' 'G35'\n",
      " 'G36' 'G37' 'G38' 'G39' 'G40' 'G41' 'G42' 'G43' 'G44' 'G45' 'G46' 'G47'\n",
      " 'G48' 'G49' 'G50' 'G51' 'G52' 'G53' 'G54' 'G55' 'G56' 'G57' 'G58' 'G59'\n",
      " 'G60' 'G61' 'G62' 'G63' 'G64' 'G65' 'G66' 'G67' 'G68' 'G69' 'G70' 'G71'\n",
      " 'G72' 'G73' 'G74' 'G75' 'G76' 'G77' 'G78' 'G79' 'G80' 'G81' 'G82' 'G83'\n",
      " 'G84' 'G85' 'G86' 'G87' 'G88' 'G89' 'G90' 'G91' 'G92' 'G93' 'G94' 'G95'\n",
      " 'G96' 'G97' 'G98' 'G99' 'G100' 'G101' 'G102' 'G103' 'G104' 'G105' 'G106'\n",
      " 'G107' 'G108' 'G109' 'G110' 'G111' 'G112' 'G113' 'G114' 'G115' 'G116'\n",
      " 'G117' 'G118' 'G119' 'G120' 'G121' 'G122' 'G123' 'G124' 'G125' 'G126'\n",
      " 'G127' 'G128' 'G129' 'G130' 'G131' 'G132' 'G133' 'G134' 'G135' 'G136'\n",
      " 'G137' 'G138' 'G139' 'G140' 'G141' 'G142' 'G143' 'G144' 'G145' 'G146'\n",
      " 'G147' 'G148' 'G149' 'G150' 'G151' 'G152' 'G153' 'G154' 'G155' 'G156'\n",
      " 'G157' 'G158' 'G159' 'G160' 'G161' 'G162' 'G163' 'G164' 'G165' 'G166'\n",
      " 'G167' 'G168' 'G169' 'G170' 'G171' 'G172' 'G173' 'G174' 'G175' 'G176'\n",
      " 'G177' 'G178' 'G179' 'G180' 'G181' 'G182' 'G183' 'G184' 'G185' 'G186'\n",
      " 'G187' 'G188' 'G189' 'G190' 'G191' 'G192' 'G193' 'G194' 'G195' 'G196'\n",
      " 'G197' 'G198' 'G199' 'G200' 'G201' 'G202' 'G203' 'G204' 'G205' 'G206'\n",
      " 'G207' 'G208' 'G209' 'G210' 'G211' 'G212' 'G213' 'G214' 'G215' 'G216'\n",
      " 'G217' 'G218' 'G219' 'G220' 'G221' 'G222' 'G223' 'G224' 'G225' 'G226'\n",
      " 'G227' 'G228' 'G229' 'G230' 'G231' 'G232' 'G233' 'G234' 'G235' 'G236'\n",
      " 'G237' 'G238' 'G239' 'G240' 'G241' 'G242' 'G243' 'G244' 'G245' 'G246'\n",
      " 'G247' 'G248' 'G249' 'G250' 'G251' 'G252' 'G253' 'G254' 'G255' 'G256'\n",
      " 'G257' 'G258' 'G259' 'G260' 'G261' 'G262' 'G263' 'G264' 'G265' 'G266'\n",
      " 'G267' 'G268' 'G269' 'G270' 'G271' 'G272' 'G273' 'G274' 'G275' 'G276'\n",
      " 'G277' 'G278' 'G279' 'G280' 'G281' 'G282' 'G283' 'G284' 'G285' 'G286'\n",
      " 'G287' 'G288' 'G289' 'G290' 'G291' 'G292' 'G293' 'G294' 'G295' 'G296'\n",
      " 'G297' 'G298' 'G299' 'G300']\n",
      "*** Method:  LogisticRegression\n",
      "trained-acc:  95.25\n",
      "test- \n",
      "accuracy:0.545\n",
      "precision:0.5327830347144458\n",
      "recall:0.545\n",
      "f1 score:0.5384169736034378\n",
      "\n",
      "\n",
      "*** Method:  Support Vector Machines\n",
      "trained-acc:  56.25\n",
      "test- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.59\n",
      "precision:0.5469302124905621\n",
      "recall:0.59\n",
      "f1 score:0.5608035714285715\n",
      "\n",
      "\n",
      "*** Method:  KNN or k-Nearest Neighbors\n",
      "trained-acc:  70.5\n",
      "test- \n",
      "accuracy:0.515\n",
      "precision:0.5130277185501066\n",
      "recall:0.515\n",
      "f1 score:0.49641016832703544\n",
      "\n",
      "\n",
      "*** Method:  Gaussian Naive Bayes\n",
      "trained-acc:  48.25\n",
      "test- \n",
      "accuracy:0.33\n",
      "precision:0.5143341773197448\n",
      "recall:0.33\n",
      "f1 score:0.3798331280303286\n",
      "\n",
      "\n",
      "*** Method:  Perceptron\n",
      "trained-acc:  69.12\n",
      "test- \n",
      "accuracy:0.58\n",
      "precision:0.5618969555035129\n",
      "recall:0.58\n",
      "f1 score:0.5638379431695896\n",
      "\n",
      "\n",
      "*** Method:  Linear SVC\n",
      "trained-acc:  94.62\n",
      "test- \n",
      "accuracy:0.495\n",
      "precision:0.49812933301458284\n",
      "recall:0.495\n",
      "f1 score:0.4964701332698715\n",
      "\n",
      "\n",
      "*** Method:  Stochastic Gradient Descent\n",
      "trained-acc:  77.75\n",
      "test- \n",
      "accuracy:0.56\n",
      "precision:0.5630086613949254\n",
      "recall:0.56\n",
      "f1 score:0.5606493837598778\n",
      "\n",
      "\n",
      "*** Method:  Decision Tree\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.42\n",
      "precision:0.44583333333333336\n",
      "recall:0.42\n",
      "f1 score:0.4283130207685612\n",
      "\n",
      "\n",
      "*** Method:  Random Forest\n",
      "trained-acc:  99.88\n",
      "test- \n",
      "accuracy:0.52\n",
      "precision:0.48332694828107675\n",
      "recall:0.52\n",
      "f1 score:0.49839339991315673\n",
      "\n",
      "\n",
      "*** Method:  Ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.56\n",
      "precision:0.5195588235294117\n",
      "recall:0.56\n",
      "f1 score:0.5381746031746032\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Add Time data\n",
    "\n",
    "1) outlier value drop\n",
    "2) clustering from step 0-N days to step 0-N/365 days\n",
    "'''\n",
    "\n",
    "'''\n",
    "1)\n",
    "'''\n",
    "print('outlier of time: ')\n",
    "print(df1.loc[df1['time'] < 0, 'time'], end='\\n\\n')\n",
    "\n",
    "df1_outlier = df1.copy()\n",
    "df1_outlier.loc[df1_outlier['time'] < 0, 'time'] = abs(df1_outlier.loc[df1_outlier['time'] < 0, 'time'])\n",
    "print(df1_outlier.describe(), end='\\n\\n')\n",
    "\n",
    "df = pd.concat([df5, df1_outlier,df2,df3,df4], axis=1)\n",
    "df = df.drop(['event', 'Treatment'], axis=1)\n",
    "\n",
    "print(df.columns.values)\n",
    "\n",
    "train_df, test_df = df.iloc[random_index[:800],:], df.iloc[random_index[800:1000],:]\n",
    "\n",
    "X_train = train_df.drop(['newlabel'], axis=1)\n",
    "Y_train = train_df['newlabel']\n",
    "\n",
    "X_test = test_df.drop(['newlabel'], axis=1)\n",
    "Y_test = test_df['newlabel']\n",
    "\n",
    "\n",
    "'''\n",
    "Add Time data(1) Training & Test\n",
    "\n",
    "'''\n",
    "\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "technical-might",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              time        event\n",
      "count  1000.000000  1000.000000\n",
      "mean      0.142126     0.891000\n",
      "std       0.060610     0.311795\n",
      "min       0.019372     0.000000\n",
      "25%       0.102469     1.000000\n",
      "50%       0.128944     1.000000\n",
      "75%       0.167031     1.000000\n",
      "max       0.594737     1.000000\n",
      "\n",
      "['newlabel' 'time' 'Var1' 'Var2' 'Var3' 'Var4' 'Var5' 'Var6' 'Var7' 'Var8'\n",
      " 'Var9' 'Var10' 'G1' 'G2' 'G3' 'G4' 'G5' 'G6' 'G7' 'G8' 'G9' 'G10' 'G11'\n",
      " 'G12' 'G13' 'G14' 'G15' 'G16' 'G17' 'G18' 'G19' 'G20' 'G21' 'G22' 'G23'\n",
      " 'G24' 'G25' 'G26' 'G27' 'G28' 'G29' 'G30' 'G31' 'G32' 'G33' 'G34' 'G35'\n",
      " 'G36' 'G37' 'G38' 'G39' 'G40' 'G41' 'G42' 'G43' 'G44' 'G45' 'G46' 'G47'\n",
      " 'G48' 'G49' 'G50' 'G51' 'G52' 'G53' 'G54' 'G55' 'G56' 'G57' 'G58' 'G59'\n",
      " 'G60' 'G61' 'G62' 'G63' 'G64' 'G65' 'G66' 'G67' 'G68' 'G69' 'G70' 'G71'\n",
      " 'G72' 'G73' 'G74' 'G75' 'G76' 'G77' 'G78' 'G79' 'G80' 'G81' 'G82' 'G83'\n",
      " 'G84' 'G85' 'G86' 'G87' 'G88' 'G89' 'G90' 'G91' 'G92' 'G93' 'G94' 'G95'\n",
      " 'G96' 'G97' 'G98' 'G99' 'G100' 'G101' 'G102' 'G103' 'G104' 'G105' 'G106'\n",
      " 'G107' 'G108' 'G109' 'G110' 'G111' 'G112' 'G113' 'G114' 'G115' 'G116'\n",
      " 'G117' 'G118' 'G119' 'G120' 'G121' 'G122' 'G123' 'G124' 'G125' 'G126'\n",
      " 'G127' 'G128' 'G129' 'G130' 'G131' 'G132' 'G133' 'G134' 'G135' 'G136'\n",
      " 'G137' 'G138' 'G139' 'G140' 'G141' 'G142' 'G143' 'G144' 'G145' 'G146'\n",
      " 'G147' 'G148' 'G149' 'G150' 'G151' 'G152' 'G153' 'G154' 'G155' 'G156'\n",
      " 'G157' 'G158' 'G159' 'G160' 'G161' 'G162' 'G163' 'G164' 'G165' 'G166'\n",
      " 'G167' 'G168' 'G169' 'G170' 'G171' 'G172' 'G173' 'G174' 'G175' 'G176'\n",
      " 'G177' 'G178' 'G179' 'G180' 'G181' 'G182' 'G183' 'G184' 'G185' 'G186'\n",
      " 'G187' 'G188' 'G189' 'G190' 'G191' 'G192' 'G193' 'G194' 'G195' 'G196'\n",
      " 'G197' 'G198' 'G199' 'G200' 'G201' 'G202' 'G203' 'G204' 'G205' 'G206'\n",
      " 'G207' 'G208' 'G209' 'G210' 'G211' 'G212' 'G213' 'G214' 'G215' 'G216'\n",
      " 'G217' 'G218' 'G219' 'G220' 'G221' 'G222' 'G223' 'G224' 'G225' 'G226'\n",
      " 'G227' 'G228' 'G229' 'G230' 'G231' 'G232' 'G233' 'G234' 'G235' 'G236'\n",
      " 'G237' 'G238' 'G239' 'G240' 'G241' 'G242' 'G243' 'G244' 'G245' 'G246'\n",
      " 'G247' 'G248' 'G249' 'G250' 'G251' 'G252' 'G253' 'G254' 'G255' 'G256'\n",
      " 'G257' 'G258' 'G259' 'G260' 'G261' 'G262' 'G263' 'G264' 'G265' 'G266'\n",
      " 'G267' 'G268' 'G269' 'G270' 'G271' 'G272' 'G273' 'G274' 'G275' 'G276'\n",
      " 'G277' 'G278' 'G279' 'G280' 'G281' 'G282' 'G283' 'G284' 'G285' 'G286'\n",
      " 'G287' 'G288' 'G289' 'G290' 'G291' 'G292' 'G293' 'G294' 'G295' 'G296'\n",
      " 'G297' 'G298' 'G299' 'G300']\n",
      "*** Method:  LogisticRegression\n",
      "trained-acc:  86.5\n",
      "test- \n",
      "accuracy:0.445\n",
      "precision:0.44675892857142857\n",
      "recall:0.445\n",
      "f1 score:0.4443221477836407\n",
      "\n",
      "\n",
      "*** Method:  Support Vector Machines\n",
      "trained-acc:  70.0\n",
      "test- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.485\n",
      "precision:0.44589538739825146\n",
      "recall:0.485\n",
      "f1 score:0.46459803921568626\n",
      "\n",
      "\n",
      "*** Method:  KNN or k-Nearest Neighbors\n",
      "trained-acc:  65.75\n",
      "test- \n",
      "accuracy:0.5\n",
      "precision:0.4969140669966617\n",
      "recall:0.5\n",
      "f1 score:0.486035489924125\n",
      "\n",
      "\n",
      "*** Method:  Gaussian Naive Bayes\n",
      "trained-acc:  39.12\n",
      "test- \n",
      "accuracy:0.315\n",
      "precision:0.5325861197941335\n",
      "recall:0.315\n",
      "f1 score:0.37115562210389796\n",
      "\n",
      "\n",
      "*** Method:  Perceptron\n",
      "trained-acc:  77.12\n",
      "test- \n",
      "accuracy:0.39\n",
      "precision:0.4338712724057552\n",
      "recall:0.39\n",
      "f1 score:0.4107184265010352\n",
      "\n",
      "\n",
      "*** Method:  Linear SVC\n",
      "trained-acc:  89.62\n",
      "test- \n",
      "accuracy:0.46\n",
      "precision:0.4750069752694991\n",
      "recall:0.46\n",
      "f1 score:0.46736213235294133\n",
      "\n",
      "\n",
      "*** Method:  Stochastic Gradient Descent\n",
      "trained-acc:  78.12\n",
      "test- \n",
      "accuracy:0.39\n",
      "precision:0.4340350877192982\n",
      "recall:0.39\n",
      "f1 score:0.40531288951738115\n",
      "\n",
      "\n",
      "*** Method:  Decision Tree\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.415\n",
      "precision:0.4273839009287926\n",
      "recall:0.415\n",
      "f1 score:0.4198351648351648\n",
      "\n",
      "\n",
      "*** Method:  Random Forest\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.48\n",
      "precision:0.4444642857142857\n",
      "recall:0.48\n",
      "f1 score:0.45803938611978817\n",
      "\n",
      "\n",
      "*** Method:  Ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.56\n",
      "precision:0.5207430513288699\n",
      "recall:0.56\n",
      "f1 score:0.5377324788655576\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2)\n",
    "'''\n",
    "df1_normalize = df1_outlier.copy()\n",
    "df1_normalize['time'] = df1_normalize['time']/365.0\n",
    "print(df1_normalize.describe(), end='\\n\\n')\n",
    "\n",
    "df = pd.concat([df5, df1_normalize,df2,df3,df4], axis=1)\n",
    "df = df.drop(['event', 'Treatment'], axis=1)\n",
    "\n",
    "print(df.columns.values)\n",
    "\n",
    "train_df, test_df = df.iloc[random_index[:800],:], df.iloc[random_index[800:1000],:]\n",
    "\n",
    "X_train = train_df.drop(['newlabel'], axis=1)\n",
    "Y_train = train_df['newlabel']\n",
    "\n",
    "X_test = test_df.drop(['newlabel'], axis=1)\n",
    "Y_test = test_df['newlabel']\n",
    "\n",
    "\n",
    "'''\n",
    "Add Time data(1) Training & Test\n",
    "\n",
    "'''\n",
    "\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "usual-church",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Var1\n",
      "2    235\n",
      "3    204\n",
      "1    171\n",
      "4    139\n",
      "5     95\n",
      "0     57\n",
      "6     50\n",
      "7     27\n",
      "8     13\n",
      "9      9\n",
      "Name: Var1, dtype: int64\n",
      "--------------------\n",
      "# Var2\n",
      "3    221\n",
      "2    218\n",
      "4    163\n",
      "1    113\n",
      "5    109\n",
      "6     65\n",
      "0     48\n",
      "7     30\n",
      "9     20\n",
      "8     13\n",
      "Name: Var2, dtype: int64\n",
      "--------------------\n",
      "# Var3\n",
      "2    260\n",
      "3    196\n",
      "1    156\n",
      "4    130\n",
      "5     97\n",
      "0     55\n",
      "6     55\n",
      "7     23\n",
      "8     16\n",
      "9     12\n",
      "Name: Var3, dtype: int64\n",
      "--------------------\n",
      "# Var4\n",
      "2    242\n",
      "3    195\n",
      "1    150\n",
      "4    140\n",
      "5    106\n",
      "6     67\n",
      "0     36\n",
      "7     32\n",
      "8     16\n",
      "9     16\n",
      "Name: Var4, dtype: int64\n",
      "--------------------\n",
      "# Var5\n",
      "2    247\n",
      "3    223\n",
      "4    161\n",
      "5    124\n",
      "1     76\n",
      "6     63\n",
      "7     41\n",
      "0     28\n",
      "9     19\n",
      "8     18\n",
      "Name: Var5, dtype: int64\n",
      "--------------------\n",
      "# Var6\n",
      "2    240\n",
      "3    212\n",
      "4    128\n",
      "1    127\n",
      "5     99\n",
      "6     64\n",
      "0     53\n",
      "7     40\n",
      "8     20\n",
      "9     17\n",
      "Name: Var6, dtype: int64\n",
      "--------------------\n",
      "# Var7\n",
      "1    269\n",
      "2    208\n",
      "3    144\n",
      "0    128\n",
      "4    118\n",
      "5     62\n",
      "6     47\n",
      "7     16\n",
      "8      6\n",
      "9      2\n",
      "Name: Var7, dtype: int64\n",
      "--------------------\n",
      "# Var8\n",
      "1    241\n",
      "2    227\n",
      "3    171\n",
      "0    127\n",
      "4    101\n",
      "5     69\n",
      "6     30\n",
      "7     20\n",
      "8     10\n",
      "9      4\n",
      "Name: Var8, dtype: int64\n",
      "--------------------\n",
      "# Var9\n",
      "2    250\n",
      "1    235\n",
      "3    157\n",
      "0    135\n",
      "4    100\n",
      "5     57\n",
      "6     33\n",
      "7     22\n",
      "9      6\n",
      "8      5\n",
      "Name: Var9, dtype: int64\n",
      "--------------------\n",
      "# Var10\n",
      "1    287\n",
      "2    205\n",
      "3    156\n",
      "0    114\n",
      "4    108\n",
      "5     62\n",
      "6     40\n",
      "7     18\n",
      "8      5\n",
      "9      5\n",
      "Name: Var10, dtype: int64\n",
      "--------------------\n",
      "['newlabel' 'time' 'Var1' 'Var2' 'Var3' 'Var4' 'Var5' 'Var6' 'Var7' 'Var8'\n",
      " 'Var9' 'Var10' 'G1' 'G2' 'G3' 'G4' 'G5' 'G6' 'G7' 'G8' 'G9' 'G10' 'G11'\n",
      " 'G12' 'G13' 'G14' 'G15' 'G16' 'G17' 'G18' 'G19' 'G20' 'G21' 'G22' 'G23'\n",
      " 'G24' 'G25' 'G26' 'G27' 'G28' 'G29' 'G30' 'G31' 'G32' 'G33' 'G34' 'G35'\n",
      " 'G36' 'G37' 'G38' 'G39' 'G40' 'G41' 'G42' 'G43' 'G44' 'G45' 'G46' 'G47'\n",
      " 'G48' 'G49' 'G50' 'G51' 'G52' 'G53' 'G54' 'G55' 'G56' 'G57' 'G58' 'G59'\n",
      " 'G60' 'G61' 'G62' 'G63' 'G64' 'G65' 'G66' 'G67' 'G68' 'G69' 'G70' 'G71'\n",
      " 'G72' 'G73' 'G74' 'G75' 'G76' 'G77' 'G78' 'G79' 'G80' 'G81' 'G82' 'G83'\n",
      " 'G84' 'G85' 'G86' 'G87' 'G88' 'G89' 'G90' 'G91' 'G92' 'G93' 'G94' 'G95'\n",
      " 'G96' 'G97' 'G98' 'G99' 'G100' 'G101' 'G102' 'G103' 'G104' 'G105' 'G106'\n",
      " 'G107' 'G108' 'G109' 'G110' 'G111' 'G112' 'G113' 'G114' 'G115' 'G116'\n",
      " 'G117' 'G118' 'G119' 'G120' 'G121' 'G122' 'G123' 'G124' 'G125' 'G126'\n",
      " 'G127' 'G128' 'G129' 'G130' 'G131' 'G132' 'G133' 'G134' 'G135' 'G136'\n",
      " 'G137' 'G138' 'G139' 'G140' 'G141' 'G142' 'G143' 'G144' 'G145' 'G146'\n",
      " 'G147' 'G148' 'G149' 'G150' 'G151' 'G152' 'G153' 'G154' 'G155' 'G156'\n",
      " 'G157' 'G158' 'G159' 'G160' 'G161' 'G162' 'G163' 'G164' 'G165' 'G166'\n",
      " 'G167' 'G168' 'G169' 'G170' 'G171' 'G172' 'G173' 'G174' 'G175' 'G176'\n",
      " 'G177' 'G178' 'G179' 'G180' 'G181' 'G182' 'G183' 'G184' 'G185' 'G186'\n",
      " 'G187' 'G188' 'G189' 'G190' 'G191' 'G192' 'G193' 'G194' 'G195' 'G196'\n",
      " 'G197' 'G198' 'G199' 'G200' 'G201' 'G202' 'G203' 'G204' 'G205' 'G206'\n",
      " 'G207' 'G208' 'G209' 'G210' 'G211' 'G212' 'G213' 'G214' 'G215' 'G216'\n",
      " 'G217' 'G218' 'G219' 'G220' 'G221' 'G222' 'G223' 'G224' 'G225' 'G226'\n",
      " 'G227' 'G228' 'G229' 'G230' 'G231' 'G232' 'G233' 'G234' 'G235' 'G236'\n",
      " 'G237' 'G238' 'G239' 'G240' 'G241' 'G242' 'G243' 'G244' 'G245' 'G246'\n",
      " 'G247' 'G248' 'G249' 'G250' 'G251' 'G252' 'G253' 'G254' 'G255' 'G256'\n",
      " 'G257' 'G258' 'G259' 'G260' 'G261' 'G262' 'G263' 'G264' 'G265' 'G266'\n",
      " 'G267' 'G268' 'G269' 'G270' 'G271' 'G272' 'G273' 'G274' 'G275' 'G276'\n",
      " 'G277' 'G278' 'G279' 'G280' 'G281' 'G282' 'G283' 'G284' 'G285' 'G286'\n",
      " 'G287' 'G288' 'G289' 'G290' 'G291' 'G292' 'G293' 'G294' 'G295' 'G296'\n",
      " 'G297' 'G298' 'G299' 'G300']\n",
      "\n",
      "*** Method:  LogisticRegression\n",
      "trained-acc:  95.12\n",
      "test- \n",
      "accuracy:0.545\n",
      "precision:0.5327830347144458\n",
      "recall:0.545\n",
      "f1 score:0.5384169736034378\n",
      "\n",
      "\n",
      "*** Method:  Support Vector Machines\n",
      "trained-acc:  56.25\n",
      "test- \n",
      "accuracy:0.59\n",
      "precision:0.5469302124905621\n",
      "recall:0.59\n",
      "f1 score:0.5608035714285715\n",
      "\n",
      "\n",
      "*** Method:  KNN or k-Nearest Neighbors\n",
      "trained-acc:  70.38\n",
      "test- \n",
      "accuracy:0.52\n",
      "precision:0.5177207843137255\n",
      "recall:0.52\n",
      "f1 score:0.502085477415666\n",
      "\n",
      "\n",
      "*** Method:  Gaussian Naive Bayes\n",
      "trained-acc:  48.0\n",
      "test- \n",
      "accuracy:0.33\n",
      "precision:0.5143341773197448\n",
      "recall:0.33\n",
      "f1 score:0.3798331280303286\n",
      "\n",
      "\n",
      "*** Method:  Perceptron\n",
      "trained-acc:  52.5\n",
      "test- \n",
      "accuracy:0.53\n",
      "precision:0.5226420454545455\n",
      "recall:0.53\n",
      "f1 score:0.44090090090090084\n",
      "\n",
      "\n",
      "*** Method:  Linear SVC\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained-acc:  94.62\n",
      "test- \n",
      "accuracy:0.5\n",
      "precision:0.5063586413586414\n",
      "recall:0.5\n",
      "f1 score:0.5027788782259257\n",
      "\n",
      "\n",
      "*** Method:  Stochastic Gradient Descent\n",
      "trained-acc:  76.25\n",
      "test- \n",
      "accuracy:0.57\n",
      "precision:0.5448870043000478\n",
      "recall:0.57\n",
      "f1 score:0.5542871556856019\n",
      "\n",
      "\n",
      "*** Method:  Decision Tree\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.455\n",
      "precision:0.46654750123701133\n",
      "recall:0.455\n",
      "f1 score:0.4591440992663708\n",
      "\n",
      "\n",
      "*** Method:  Random Forest\n",
      "trained-acc:  99.75\n",
      "test- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.49\n",
      "precision:0.45219548872180454\n",
      "recall:0.49\n",
      "f1 score:0.4690625\n",
      "\n",
      "\n",
      "*** Method:  Ensemble\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.545\n",
      "precision:0.5287424629535002\n",
      "recall:0.545\n",
      "f1 score:0.5284089912280702\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Correlating numerical features of Clinic data\n",
    "\n",
    "- outlier value drop\n",
    "- clustering from step 0-9 to step 0-4(clustering 2 steps)/0-1(just resizing)\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "1)\n",
    "'''\n",
    "df3_outlier = df3.copy()\n",
    "\n",
    "\n",
    "# drop outlier\n",
    "for col in df3_outlier.columns:\n",
    "    for outlier in range(10,13):\n",
    "        df3_outlier = df3_outlier.replace(outlier, 9)\n",
    "\n",
    "# visualize\n",
    "for col in df3_outlier.columns:\n",
    "    print('#', col)\n",
    "    print(df3_outlier[col].value_counts())\n",
    "    print('-'*20)\n",
    "\n",
    "\n",
    "df = pd.concat([df5, df1_outlier,df2,df3_outlier,df4], axis=1)\n",
    "df = df.drop(['event', 'Treatment'], axis=1)\n",
    "\n",
    "print(df.columns.values)\n",
    "\n",
    "train_df, test_df = df.iloc[random_index[:800],:], df.iloc[random_index[800:1000],:]\n",
    "\n",
    "X_train = train_df.drop(['newlabel'], axis=1)\n",
    "Y_train = train_df['newlabel']\n",
    "\n",
    "X_test = test_df.drop(['newlabel'], axis=1)\n",
    "Y_test = test_df['newlabel']\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Add Time data(1) +  Correlating Clinic data(1) \n",
    "Training & Test\n",
    "'''\n",
    "\n",
    "print()\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "conscious-large",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Var1         Var2         Var3         Var4         Var5  \\\n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
      "mean      1.226000     1.400000     1.256000     1.354000     1.500000   \n",
      "std       0.958041     0.975167     0.968195     0.995829     0.942809   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "50%       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "75%       2.000000     2.000000     2.000000     2.000000     2.000000   \n",
      "max       4.000000     4.000000     4.000000     4.000000     4.000000   \n",
      "\n",
      "              Var6         Var7         Var8         Var9        Var10  \n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
      "mean      1.366000     0.933000     0.944000     0.930000     0.915000  \n",
      "std       1.011468     0.947316     0.928292     0.917574     0.941627  \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
      "25%       1.000000     0.000000     0.000000     0.000000     0.000000  \n",
      "50%       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
      "75%       2.000000     2.000000     1.000000     1.000000     1.000000  \n",
      "max       4.000000     4.000000     4.000000     4.000000     4.000000  \n",
      "\n",
      "['newlabel' 'time' 'Var1' 'Var2' 'Var3' 'Var4' 'Var5' 'Var6' 'Var7' 'Var8'\n",
      " 'Var9' 'Var10' 'G1' 'G2' 'G3' 'G4' 'G5' 'G6' 'G7' 'G8' 'G9' 'G10' 'G11'\n",
      " 'G12' 'G13' 'G14' 'G15' 'G16' 'G17' 'G18' 'G19' 'G20' 'G21' 'G22' 'G23'\n",
      " 'G24' 'G25' 'G26' 'G27' 'G28' 'G29' 'G30' 'G31' 'G32' 'G33' 'G34' 'G35'\n",
      " 'G36' 'G37' 'G38' 'G39' 'G40' 'G41' 'G42' 'G43' 'G44' 'G45' 'G46' 'G47'\n",
      " 'G48' 'G49' 'G50' 'G51' 'G52' 'G53' 'G54' 'G55' 'G56' 'G57' 'G58' 'G59'\n",
      " 'G60' 'G61' 'G62' 'G63' 'G64' 'G65' 'G66' 'G67' 'G68' 'G69' 'G70' 'G71'\n",
      " 'G72' 'G73' 'G74' 'G75' 'G76' 'G77' 'G78' 'G79' 'G80' 'G81' 'G82' 'G83'\n",
      " 'G84' 'G85' 'G86' 'G87' 'G88' 'G89' 'G90' 'G91' 'G92' 'G93' 'G94' 'G95'\n",
      " 'G96' 'G97' 'G98' 'G99' 'G100' 'G101' 'G102' 'G103' 'G104' 'G105' 'G106'\n",
      " 'G107' 'G108' 'G109' 'G110' 'G111' 'G112' 'G113' 'G114' 'G115' 'G116'\n",
      " 'G117' 'G118' 'G119' 'G120' 'G121' 'G122' 'G123' 'G124' 'G125' 'G126'\n",
      " 'G127' 'G128' 'G129' 'G130' 'G131' 'G132' 'G133' 'G134' 'G135' 'G136'\n",
      " 'G137' 'G138' 'G139' 'G140' 'G141' 'G142' 'G143' 'G144' 'G145' 'G146'\n",
      " 'G147' 'G148' 'G149' 'G150' 'G151' 'G152' 'G153' 'G154' 'G155' 'G156'\n",
      " 'G157' 'G158' 'G159' 'G160' 'G161' 'G162' 'G163' 'G164' 'G165' 'G166'\n",
      " 'G167' 'G168' 'G169' 'G170' 'G171' 'G172' 'G173' 'G174' 'G175' 'G176'\n",
      " 'G177' 'G178' 'G179' 'G180' 'G181' 'G182' 'G183' 'G184' 'G185' 'G186'\n",
      " 'G187' 'G188' 'G189' 'G190' 'G191' 'G192' 'G193' 'G194' 'G195' 'G196'\n",
      " 'G197' 'G198' 'G199' 'G200' 'G201' 'G202' 'G203' 'G204' 'G205' 'G206'\n",
      " 'G207' 'G208' 'G209' 'G210' 'G211' 'G212' 'G213' 'G214' 'G215' 'G216'\n",
      " 'G217' 'G218' 'G219' 'G220' 'G221' 'G222' 'G223' 'G224' 'G225' 'G226'\n",
      " 'G227' 'G228' 'G229' 'G230' 'G231' 'G232' 'G233' 'G234' 'G235' 'G236'\n",
      " 'G237' 'G238' 'G239' 'G240' 'G241' 'G242' 'G243' 'G244' 'G245' 'G246'\n",
      " 'G247' 'G248' 'G249' 'G250' 'G251' 'G252' 'G253' 'G254' 'G255' 'G256'\n",
      " 'G257' 'G258' 'G259' 'G260' 'G261' 'G262' 'G263' 'G264' 'G265' 'G266'\n",
      " 'G267' 'G268' 'G269' 'G270' 'G271' 'G272' 'G273' 'G274' 'G275' 'G276'\n",
      " 'G277' 'G278' 'G279' 'G280' 'G281' 'G282' 'G283' 'G284' 'G285' 'G286'\n",
      " 'G287' 'G288' 'G289' 'G290' 'G291' 'G292' 'G293' 'G294' 'G295' 'G296'\n",
      " 'G297' 'G298' 'G299' 'G300']\n",
      "*** Method:  LogisticRegression\n",
      "trained-acc:  93.5\n",
      "test- \n",
      "accuracy:0.515\n",
      "precision:0.5085625\n",
      "recall:0.515\n",
      "f1 score:0.5116168814061147\n",
      "\n",
      "\n",
      "*** Method:  Support Vector Machines\n",
      "trained-acc:  54.37\n",
      "test- \n",
      "accuracy:0.58\n",
      "precision:0.5379080685664374\n",
      "recall:0.58\n",
      "f1 score:0.5500907359695306\n",
      "\n",
      "\n",
      "*** Method:  KNN or k-Nearest Neighbors\n",
      "trained-acc:  68.25\n",
      "test- \n",
      "accuracy:0.525\n",
      "precision:0.49524708377518567\n",
      "recall:0.525\n",
      "f1 score:0.5033826539078489\n",
      "\n",
      "\n",
      "*** Method:  Gaussian Naive Bayes\n",
      "trained-acc:  47.75\n",
      "test- \n",
      "accuracy:0.32\n",
      "precision:0.5064938039995858\n",
      "recall:0.32\n",
      "f1 score:0.369404347431201\n",
      "\n",
      "\n",
      "*** Method:  Perceptron\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained-acc:  66.38\n",
      "test- \n",
      "accuracy:0.52\n",
      "precision:0.5057783564814815\n",
      "recall:0.52\n",
      "f1 score:0.49137629269565786\n",
      "\n",
      "\n",
      "*** Method:  Linear SVC\n",
      "trained-acc:  92.88\n",
      "test- \n",
      "accuracy:0.465\n",
      "precision:0.4777515381519176\n",
      "recall:0.465\n",
      "f1 score:0.470891071728569\n",
      "\n",
      "\n",
      "*** Method:  Stochastic Gradient Descent\n",
      "trained-acc:  75.88\n",
      "test- \n",
      "accuracy:0.54\n",
      "precision:0.5447348484848484\n",
      "recall:0.54\n",
      "f1 score:0.5127726219490234\n",
      "\n",
      "\n",
      "*** Method:  Decision Tree\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.475\n",
      "precision:0.4772879365605659\n",
      "recall:0.475\n",
      "f1 score:0.4743722568180965\n",
      "\n",
      "\n",
      "*** Method:  Random Forest\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.48\n",
      "precision:0.44355906468266015\n",
      "recall:0.48\n",
      "f1 score:0.4579472140762463\n",
      "\n",
      "\n",
      "*** Method:  Ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.555\n",
      "precision:0.5178071253071254\n",
      "recall:0.555\n",
      "f1 score:0.5318910728910728\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2-1) 2 steps clustering(0-4)\n",
    "'''\n",
    "df3_2c = df3_outlier.copy()\n",
    "\n",
    "for col in df3_2c.columns:\n",
    "    df3_2c.loc[df3_2c[col] <= 1, col] = 0\n",
    "    df3_2c.loc[(df3_2c[col] > 1) & (df3_2c[col] <= 3), col] = 1\n",
    "    df3_2c.loc[(df3_2c[col] > 3) & (df3_2c[col] <= 5), col] = 2\n",
    "    df3_2c.loc[(df3_2c[col] > 5) & (df3_2c[col] <= 7), col] = 3\n",
    "    df3_2c.loc[(df3_2c[col] > 7) & (df3_2c[col] <= 9), col] = 4\n",
    "\n",
    "print(df3_2c.describe(), end='\\n\\n')\n",
    "\n",
    "df = pd.concat([df5, df1_outlier,df2,df3_2c,df4], axis=1)\n",
    "df = df.drop(['event', 'Treatment'], axis=1)\n",
    "\n",
    "print(df.columns.values)\n",
    "\n",
    "train_df, test_df = df.iloc[random_index[:800],:], df.iloc[random_index[800:1000],:]\n",
    "\n",
    "X_train = train_df.drop(['newlabel'], axis=1)\n",
    "Y_train = train_df['newlabel']\n",
    "\n",
    "X_test = test_df.drop(['newlabel'], axis=1)\n",
    "Y_test = test_df['newlabel']\n",
    "\n",
    "\n",
    "'''\n",
    "Add Time data(1) +  Correlating Clinic data(2-1) \n",
    "Training & Test\n",
    "'''\n",
    "\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "colonial-maryland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Var1         Var2         Var3         Var4         Var5  \\\n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000   \n",
      "mean      0.328667     0.365889     0.332889     0.356333     0.387000   \n",
      "std       0.207386     0.214559     0.210689     0.216080     0.210446   \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
      "25%       0.222222     0.222222     0.222222     0.222222     0.222222   \n",
      "50%       0.333333     0.333333     0.333333     0.333333     0.333333   \n",
      "75%       0.444444     0.444444     0.444444     0.444444     0.555556   \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
      "\n",
      "              Var6         Var7         Var8         Var9        Var10  \n",
      "count  1000.000000  1000.000000  1000.000000  1000.000000  1000.000000  \n",
      "mean      0.358556     0.262111     0.265889     0.259667     0.262000  \n",
      "std       0.223040     0.201703     0.202755     0.201437     0.201204  \n",
      "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
      "25%       0.222222     0.111111     0.111111     0.111111     0.111111  \n",
      "50%       0.333333     0.222222     0.222222     0.222222     0.222222  \n",
      "75%       0.444444     0.444444     0.333333     0.333333     0.333333  \n",
      "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
      "\n",
      "['newlabel' 'time' 'Var1' 'Var2' 'Var3' 'Var4' 'Var5' 'Var6' 'Var7' 'Var8'\n",
      " 'Var9' 'Var10' 'G1' 'G2' 'G3' 'G4' 'G5' 'G6' 'G7' 'G8' 'G9' 'G10' 'G11'\n",
      " 'G12' 'G13' 'G14' 'G15' 'G16' 'G17' 'G18' 'G19' 'G20' 'G21' 'G22' 'G23'\n",
      " 'G24' 'G25' 'G26' 'G27' 'G28' 'G29' 'G30' 'G31' 'G32' 'G33' 'G34' 'G35'\n",
      " 'G36' 'G37' 'G38' 'G39' 'G40' 'G41' 'G42' 'G43' 'G44' 'G45' 'G46' 'G47'\n",
      " 'G48' 'G49' 'G50' 'G51' 'G52' 'G53' 'G54' 'G55' 'G56' 'G57' 'G58' 'G59'\n",
      " 'G60' 'G61' 'G62' 'G63' 'G64' 'G65' 'G66' 'G67' 'G68' 'G69' 'G70' 'G71'\n",
      " 'G72' 'G73' 'G74' 'G75' 'G76' 'G77' 'G78' 'G79' 'G80' 'G81' 'G82' 'G83'\n",
      " 'G84' 'G85' 'G86' 'G87' 'G88' 'G89' 'G90' 'G91' 'G92' 'G93' 'G94' 'G95'\n",
      " 'G96' 'G97' 'G98' 'G99' 'G100' 'G101' 'G102' 'G103' 'G104' 'G105' 'G106'\n",
      " 'G107' 'G108' 'G109' 'G110' 'G111' 'G112' 'G113' 'G114' 'G115' 'G116'\n",
      " 'G117' 'G118' 'G119' 'G120' 'G121' 'G122' 'G123' 'G124' 'G125' 'G126'\n",
      " 'G127' 'G128' 'G129' 'G130' 'G131' 'G132' 'G133' 'G134' 'G135' 'G136'\n",
      " 'G137' 'G138' 'G139' 'G140' 'G141' 'G142' 'G143' 'G144' 'G145' 'G146'\n",
      " 'G147' 'G148' 'G149' 'G150' 'G151' 'G152' 'G153' 'G154' 'G155' 'G156'\n",
      " 'G157' 'G158' 'G159' 'G160' 'G161' 'G162' 'G163' 'G164' 'G165' 'G166'\n",
      " 'G167' 'G168' 'G169' 'G170' 'G171' 'G172' 'G173' 'G174' 'G175' 'G176'\n",
      " 'G177' 'G178' 'G179' 'G180' 'G181' 'G182' 'G183' 'G184' 'G185' 'G186'\n",
      " 'G187' 'G188' 'G189' 'G190' 'G191' 'G192' 'G193' 'G194' 'G195' 'G196'\n",
      " 'G197' 'G198' 'G199' 'G200' 'G201' 'G202' 'G203' 'G204' 'G205' 'G206'\n",
      " 'G207' 'G208' 'G209' 'G210' 'G211' 'G212' 'G213' 'G214' 'G215' 'G216'\n",
      " 'G217' 'G218' 'G219' 'G220' 'G221' 'G222' 'G223' 'G224' 'G225' 'G226'\n",
      " 'G227' 'G228' 'G229' 'G230' 'G231' 'G232' 'G233' 'G234' 'G235' 'G236'\n",
      " 'G237' 'G238' 'G239' 'G240' 'G241' 'G242' 'G243' 'G244' 'G245' 'G246'\n",
      " 'G247' 'G248' 'G249' 'G250' 'G251' 'G252' 'G253' 'G254' 'G255' 'G256'\n",
      " 'G257' 'G258' 'G259' 'G260' 'G261' 'G262' 'G263' 'G264' 'G265' 'G266'\n",
      " 'G267' 'G268' 'G269' 'G270' 'G271' 'G272' 'G273' 'G274' 'G275' 'G276'\n",
      " 'G277' 'G278' 'G279' 'G280' 'G281' 'G282' 'G283' 'G284' 'G285' 'G286'\n",
      " 'G287' 'G288' 'G289' 'G290' 'G291' 'G292' 'G293' 'G294' 'G295' 'G296'\n",
      " 'G297' 'G298' 'G299' 'G300']\n",
      "*** Method:  LogisticRegression\n",
      "trained-acc:  92.88\n",
      "test- \n",
      "accuracy:0.53\n",
      "precision:0.5292058204768584\n",
      "recall:0.5299999999999999\n",
      "f1 score:0.528706042340488\n",
      "\n",
      "\n",
      "*** Method:  Support Vector Machines\n",
      "trained-acc:  53.12\n",
      "test- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.585\n",
      "precision:0.5442446524064171\n",
      "recall:0.585\n",
      "f1 score:0.5534275250035217\n",
      "\n",
      "\n",
      "*** Method:  KNN or k-Nearest Neighbors\n",
      "trained-acc:  65.62\n",
      "test- \n",
      "accuracy:0.5\n",
      "precision:0.49104723502304154\n",
      "recall:0.5\n",
      "f1 score:0.4814138547549451\n",
      "\n",
      "\n",
      "*** Method:  Gaussian Naive Bayes\n",
      "trained-acc:  48.0\n",
      "test- \n",
      "accuracy:0.33\n",
      "precision:0.5143341773197448\n",
      "recall:0.33\n",
      "f1 score:0.3798331280303286\n",
      "\n",
      "\n",
      "*** Method:  Perceptron\n",
      "trained-acc:  8.88\n",
      "test- \n",
      "accuracy:0.035\n",
      "precision:0.3240947546531302\n",
      "recall:0.035\n",
      "f1 score:0.020877832512315272\n",
      "\n",
      "\n",
      "*** Method:  Linear SVC\n",
      "trained-acc:  94.12\n",
      "test- \n",
      "accuracy:0.495\n",
      "precision:0.5000427350427351\n",
      "recall:0.495\n",
      "f1 score:0.4969609824225876\n",
      "\n",
      "\n",
      "*** Method:  Stochastic Gradient Descent\n",
      "trained-acc:  50.38\n",
      "test- \n",
      "accuracy:0.385\n",
      "precision:0.5617104341736695\n",
      "recall:0.385\n",
      "f1 score:0.31890818257018355\n",
      "\n",
      "\n",
      "*** Method:  Decision Tree\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.41\n",
      "precision:0.4327968460111318\n",
      "recall:0.41\n",
      "f1 score:0.4178859894377136\n",
      "\n",
      "\n",
      "*** Method:  Random Forest\n",
      "trained-acc:  99.75\n",
      "test- \n",
      "accuracy:0.53\n",
      "precision:0.4913503836317136\n",
      "recall:0.53\n",
      "f1 score:0.5046034163855946\n",
      "\n",
      "\n",
      "*** Method:  Ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.56\n",
      "precision:0.5427557603686636\n",
      "recall:0.56\n",
      "f1 score:0.5422987938596491\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2-2) just resizing(0-1)\n",
    "'''\n",
    "df3_rs = df3_outlier.copy()\n",
    "\n",
    "for col in df3_rs.columns:\n",
    "    df3_rs[col] = df3_rs[col]/9\n",
    "\n",
    "print(df3_rs.describe(), end='\\n\\n')\n",
    "\n",
    "df = pd.concat([df5, df1_outlier,df2,df3_rs,df4], axis=1)\n",
    "df = df.drop(['event', 'Treatment'], axis=1)\n",
    "\n",
    "print(df.columns.values)\n",
    "\n",
    "train_df, test_df = df.iloc[random_index[:800],:], df.iloc[random_index[800:1000],:]\n",
    "\n",
    "X_train = train_df.drop(['newlabel'], axis=1)\n",
    "Y_train = train_df['newlabel']\n",
    "\n",
    "X_test = test_df.drop(['newlabel'], axis=1)\n",
    "Y_test = test_df['newlabel']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "Add Time data(1) +  Correlating Clinic data(2-2) \n",
    "Training & Test\n",
    "'''\n",
    "\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0bea627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** train result ***\n",
      "\n",
      "                      LogisticRegression  Support Vector Machines  \\\n",
      "Basic                              85.62                    70.00   \n",
      "Time_outlier                       95.25                    56.25   \n",
      "Time_/365                          86.50                    70.00   \n",
      "Clinic_outlier                     95.12                    56.25   \n",
      "Clinic_0-4(2steps)                 93.50                    54.37   \n",
      "Clinic_0-1(resizing)               92.88                    53.12   \n",
      "\n",
      "                      KNN or k-Nearest Neighbors  Gaussian Naive Bayes  \\\n",
      "Basic                                      65.38                 38.62   \n",
      "Time_outlier                               70.50                 48.25   \n",
      "Time_/365                                  65.75                 39.12   \n",
      "Clinic_outlier                             70.38                 48.00   \n",
      "Clinic_0-4(2steps)                         68.25                 47.75   \n",
      "Clinic_0-1(resizing)                       65.62                 48.00   \n",
      "\n",
      "                      Perceptron  Linear SVC  Stochastic Gradient Descent  \\\n",
      "Basic                      64.88       86.00                        81.12   \n",
      "Time_outlier               69.12       94.62                        77.75   \n",
      "Time_/365                  77.12       89.62                        78.12   \n",
      "Clinic_outlier             52.50       94.62                        76.25   \n",
      "Clinic_0-4(2steps)         66.38       92.88                        75.88   \n",
      "Clinic_0-1(resizing)        8.88       94.12                        50.38   \n",
      "\n",
      "                      Decision Tree  Random Forest  Ensemble  \n",
      "Basic                         100.0          99.88     100.0  \n",
      "Time_outlier                  100.0          99.88     100.0  \n",
      "Time_/365                     100.0         100.00     100.0  \n",
      "Clinic_outlier                100.0          99.75     100.0  \n",
      "Clinic_0-4(2steps)            100.0         100.00     100.0  \n",
      "Clinic_0-1(resizing)          100.0          99.75     100.0  \n",
      "\n",
      "\n",
      "\n",
      "*** test acc ***\n",
      "\n",
      "                      LogisticRegression  Support Vector Machines  \\\n",
      "Basic                              0.435                    0.485   \n",
      "Time_outlier                       0.545                    0.590   \n",
      "Time_/365                          0.445                    0.485   \n",
      "Clinic_outlier                     0.545                    0.590   \n",
      "Clinic_0-4(2steps)                 0.515                    0.580   \n",
      "Clinic_0-1(resizing)               0.530                    0.585   \n",
      "\n",
      "                      KNN or k-Nearest Neighbors  Gaussian Naive Bayes  \\\n",
      "Basic                                      0.495                 0.285   \n",
      "Time_outlier                               0.515                 0.330   \n",
      "Time_/365                                  0.500                 0.315   \n",
      "Clinic_outlier                             0.520                 0.330   \n",
      "Clinic_0-4(2steps)                         0.525                 0.320   \n",
      "Clinic_0-1(resizing)                       0.500                 0.330   \n",
      "\n",
      "                      Perceptron  Linear SVC  Stochastic Gradient Descent  \\\n",
      "Basic                      0.450       0.425                        0.440   \n",
      "Time_outlier               0.580       0.495                        0.560   \n",
      "Time_/365                  0.390       0.460                        0.390   \n",
      "Clinic_outlier             0.530       0.500                        0.570   \n",
      "Clinic_0-4(2steps)         0.520       0.465                        0.540   \n",
      "Clinic_0-1(resizing)       0.035       0.495                        0.385   \n",
      "\n",
      "                      Decision Tree  Random Forest  Ensemble  \n",
      "Basic                         0.390           0.42     0.505  \n",
      "Time_outlier                  0.420           0.52     0.560  \n",
      "Time_/365                     0.415           0.48     0.560  \n",
      "Clinic_outlier                0.455           0.49     0.545  \n",
      "Clinic_0-4(2steps)            0.475           0.48     0.555  \n",
      "Clinic_0-1(resizing)          0.410           0.53     0.560  \n",
      "\n",
      "\n",
      "\n",
      "*** test pre ***\n",
      "\n",
      "                      LogisticRegression  Support Vector Machines  \\\n",
      "Basic                           0.438923                 0.445895   \n",
      "Time_outlier                    0.532783                 0.546930   \n",
      "Time_/365                       0.446759                 0.445895   \n",
      "Clinic_outlier                  0.532783                 0.546930   \n",
      "Clinic_0-4(2steps)              0.508563                 0.537908   \n",
      "Clinic_0-1(resizing)            0.529206                 0.544245   \n",
      "\n",
      "                      KNN or k-Nearest Neighbors  Gaussian Naive Bayes  \\\n",
      "Basic                                   0.490044              0.484237   \n",
      "Time_outlier                            0.513028              0.514334   \n",
      "Time_/365                               0.496914              0.532586   \n",
      "Clinic_outlier                          0.517721              0.514334   \n",
      "Clinic_0-4(2steps)                      0.495247              0.506494   \n",
      "Clinic_0-1(resizing)                    0.491047              0.514334   \n",
      "\n",
      "                      Perceptron  Linear SVC  Stochastic Gradient Descent  \\\n",
      "Basic                   0.421160    0.435927                     0.443189   \n",
      "Time_outlier            0.561897    0.498129                     0.563009   \n",
      "Time_/365               0.433871    0.475007                     0.434035   \n",
      "Clinic_outlier          0.522642    0.506359                     0.544887   \n",
      "Clinic_0-4(2steps)      0.505778    0.477752                     0.544735   \n",
      "Clinic_0-1(resizing)    0.324095    0.500043                     0.561710   \n",
      "\n",
      "                      Decision Tree  Random Forest  Ensemble  \n",
      "Basic                      0.416190       0.388630  0.465633  \n",
      "Time_outlier               0.445833       0.483327  0.519559  \n",
      "Time_/365                  0.427384       0.444464  0.520743  \n",
      "Clinic_outlier             0.466548       0.452195  0.528742  \n",
      "Clinic_0-4(2steps)         0.477288       0.443559  0.517807  \n",
      "Clinic_0-1(resizing)       0.432797       0.491350  0.542756  \n",
      "\n",
      "\n",
      "\n",
      "*** test rec ***\n",
      "\n",
      "                      LogisticRegression  Support Vector Machines  \\\n",
      "Basic                              0.435                    0.485   \n",
      "Time_outlier                       0.545                    0.590   \n",
      "Time_/365                          0.445                    0.485   \n",
      "Clinic_outlier                     0.545                    0.590   \n",
      "Clinic_0-4(2steps)                 0.515                    0.580   \n",
      "Clinic_0-1(resizing)               0.530                    0.585   \n",
      "\n",
      "                      KNN or k-Nearest Neighbors  Gaussian Naive Bayes  \\\n",
      "Basic                                      0.495                 0.285   \n",
      "Time_outlier                               0.515                 0.330   \n",
      "Time_/365                                  0.500                 0.315   \n",
      "Clinic_outlier                             0.520                 0.330   \n",
      "Clinic_0-4(2steps)                         0.525                 0.320   \n",
      "Clinic_0-1(resizing)                       0.500                 0.330   \n",
      "\n",
      "                      Perceptron  Linear SVC  Stochastic Gradient Descent  \\\n",
      "Basic                      0.450       0.425                        0.440   \n",
      "Time_outlier               0.580       0.495                        0.560   \n",
      "Time_/365                  0.390       0.460                        0.390   \n",
      "Clinic_outlier             0.530       0.500                        0.570   \n",
      "Clinic_0-4(2steps)         0.520       0.465                        0.540   \n",
      "Clinic_0-1(resizing)       0.035       0.495                        0.385   \n",
      "\n",
      "                      Decision Tree  Random Forest  Ensemble  \n",
      "Basic                         0.390           0.42     0.505  \n",
      "Time_outlier                  0.420           0.52     0.560  \n",
      "Time_/365                     0.415           0.48     0.560  \n",
      "Clinic_outlier                0.455           0.49     0.545  \n",
      "Clinic_0-4(2steps)            0.475           0.48     0.555  \n",
      "Clinic_0-1(resizing)          0.410           0.53     0.560  \n",
      "\n",
      "\n",
      "\n",
      "*** test f1 ***\n",
      "\n",
      "                      LogisticRegression  Support Vector Machines  \\\n",
      "Basic                           0.435505                 0.464598   \n",
      "Time_outlier                    0.538417                 0.560804   \n",
      "Time_/365                       0.444322                 0.464598   \n",
      "Clinic_outlier                  0.538417                 0.560804   \n",
      "Clinic_0-4(2steps)              0.511617                 0.550091   \n",
      "Clinic_0-1(resizing)            0.528706                 0.553428   \n",
      "\n",
      "                      KNN or k-Nearest Neighbors  Gaussian Naive Bayes  \\\n",
      "Basic                                   0.484129              0.333680   \n",
      "Time_outlier                            0.496410              0.379833   \n",
      "Time_/365                               0.486035              0.371156   \n",
      "Clinic_outlier                          0.502085              0.379833   \n",
      "Clinic_0-4(2steps)                      0.503383              0.369404   \n",
      "Clinic_0-1(resizing)                    0.481414              0.379833   \n",
      "\n",
      "                      Perceptron  Linear SVC  Stochastic Gradient Descent  \\\n",
      "Basic                   0.398234    0.429966                     0.436048   \n",
      "Time_outlier            0.563838    0.496470                     0.560649   \n",
      "Time_/365               0.410718    0.467362                     0.405313   \n",
      "Clinic_outlier          0.440901    0.502779                     0.554287   \n",
      "Clinic_0-4(2steps)      0.491376    0.470891                     0.512773   \n",
      "Clinic_0-1(resizing)    0.020878    0.496961                     0.318908   \n",
      "\n",
      "                      Decision Tree  Random Forest  Ensemble  \n",
      "Basic                      0.399558       0.402151  0.483851  \n",
      "Time_outlier               0.428313       0.498393  0.538175  \n",
      "Time_/365                  0.419835       0.458039  0.537732  \n",
      "Clinic_outlier             0.459144       0.469062  0.528409  \n",
      "Clinic_0-4(2steps)         0.474372       0.457947  0.531891  \n",
      "Clinic_0-1(resizing)       0.417886       0.504603  0.542299  \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('*** train result ***', end='\\n\\n')\n",
    "result_train_df.index = index\n",
    "print(result_train_df)\n",
    "print(end='\\n\\n\\n')\n",
    "\n",
    "# print('*** test result ***', end='\\n\\n')\n",
    "# result_test_df.index = index\n",
    "# print(result_test_df)\n",
    "# print(end='\\n\\n\\n')\n",
    "\n",
    "print('*** test acc ***', end='\\n\\n')\n",
    "result_acc_test_df.index = index\n",
    "print(result_acc_test_df)\n",
    "print(end='\\n\\n\\n')\n",
    "\n",
    "print('*** test pre ***', end='\\n\\n')\n",
    "result_pre_test_df.index = index\n",
    "print(result_pre_test_df)\n",
    "print(end='\\n\\n\\n')\n",
    "\n",
    "print('*** test rec ***', end='\\n\\n')\n",
    "result_rec_test_df.index = index\n",
    "print(result_rec_test_df)\n",
    "print(end='\\n\\n\\n')\n",
    "\n",
    "print('*** test f1 ***', end='\\n\\n')\n",
    "result_f1score_test_df.index = index\n",
    "print(result_f1score_test_df)\n",
    "print(end='\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99029de4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression              95.25\n",
      "Support Vector Machines         70.00\n",
      "KNN or k-Nearest Neighbors      70.50\n",
      "Gaussian Naive Bayes            48.25\n",
      "Perceptron                      77.12\n",
      "Linear SVC                      94.62\n",
      "Stochastic Gradient Descent     81.12\n",
      "Decision Tree                  100.00\n",
      "Random Forest                  100.00\n",
      "Ensemble                       100.00\n",
      "Name: max, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(result_train_df.describe().loc['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce0ea3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result_test_df.describe().loc['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "tired-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression             0.545\n",
      "Support Vector Machines        0.590\n",
      "KNN or k-Nearest Neighbors     0.525\n",
      "Gaussian Naive Bayes           0.330\n",
      "Perceptron                     0.580\n",
      "Linear SVC                     0.500\n",
      "Stochastic Gradient Descent    0.570\n",
      "Decision Tree                  0.475\n",
      "Random Forest                  0.530\n",
      "Ensemble                       0.560\n",
      "Name: max, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(result_acc_test_df.describe().loc['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "usual-regard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression             0.532783\n",
      "Support Vector Machines        0.546930\n",
      "KNN or k-Nearest Neighbors     0.517721\n",
      "Gaussian Naive Bayes           0.532586\n",
      "Perceptron                     0.561897\n",
      "Linear SVC                     0.506359\n",
      "Stochastic Gradient Descent    0.563009\n",
      "Decision Tree                  0.477288\n",
      "Random Forest                  0.491350\n",
      "Ensemble                       0.542756\n",
      "Name: max, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(result_pre_test_df.describe().loc['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "golden-level",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression             0.545\n",
      "Support Vector Machines        0.590\n",
      "KNN or k-Nearest Neighbors     0.525\n",
      "Gaussian Naive Bayes           0.330\n",
      "Perceptron                     0.580\n",
      "Linear SVC                     0.500\n",
      "Stochastic Gradient Descent    0.570\n",
      "Decision Tree                  0.475\n",
      "Random Forest                  0.530\n",
      "Ensemble                       0.560\n",
      "Name: max, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(result_rec_test_df.describe().loc['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "monthly-hungary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression             0.538417\n",
      "Support Vector Machines        0.560804\n",
      "KNN or k-Nearest Neighbors     0.503383\n",
      "Gaussian Naive Bayes           0.379833\n",
      "Perceptron                     0.563838\n",
      "Linear SVC                     0.502779\n",
      "Stochastic Gradient Descent    0.560649\n",
      "Decision Tree                  0.474372\n",
      "Random Forest                  0.504603\n",
      "Ensemble                       0.542299\n",
      "Name: max, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(result_f1score_test_df.describe().loc['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "distributed-cloud",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 적용 전 학습용 피처/레이블 데이터 세트:  (800, 311) (800,)\n",
      "SMOTE 적용 전 레이블 값 분포: \n",
      "3    354\n",
      "1    353\n",
      "2     55\n",
      "0     38\n",
      "Name: newlabel, dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SMOTE 적용 후 학습용 피처/레이블 데이터 세트:  (1416, 311) (1416,)\n",
      "SMOTE 적용 후 레이블 값 분포: \n",
      "0    354\n",
      "1    354\n",
      "2    354\n",
      "3    354\n",
      "Name: newlabel, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SMOTE at df1_outlier and df3_outlier\n",
    "\n",
    "Add Time data(1) +  Correlating Clinic data(1) \n",
    "Training & Test\n",
    "\n",
    "'''\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "df = pd.concat([df5, df1_outlier,df2,df3_outlier,df4], axis=1)\n",
    "df = df.drop(['event', 'Treatment'], axis=1)\n",
    "\n",
    "train_df, test_df = df.iloc[random_index[:800],:], df.iloc[random_index[800:1000],:]\n",
    "\n",
    "X_train = train_df.drop(['newlabel'], axis=1)\n",
    "Y_train = train_df['newlabel']\n",
    "\n",
    "X_test = test_df.drop(['newlabel'], axis=1)\n",
    "Y_test = test_df['newlabel']\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_over, Y_train_over = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "# print()\n",
    "# training()\n",
    "\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, Y_train.shape)\n",
    "print('SMOTE 적용 전 레이블 값 분포: ')\n",
    "print(pd.Series(Y_train).value_counts())\n",
    "print('-'*100)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, Y_train_over.shape)\n",
    "print('SMOTE 적용 후 레이블 값 분포: ')\n",
    "print(pd.Series(Y_train_over).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "responsible-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training():\n",
    "    result_train = []\n",
    "    result_test = []\n",
    "    result_acc = []\n",
    "    result_pre = []\n",
    "    result_rec = []\n",
    "    result_f1 = []\n",
    "    for input_method in test_method_list:\n",
    "        print('*** Method: ', input_method)\n",
    "\n",
    "        if input_method == 'LogisticRegression':\n",
    "            model = LogisticRegression(max_iter = 10000)\n",
    "\n",
    "        elif input_method == 'Support Vector Machines':\n",
    "            model = SVC()\n",
    "\n",
    "        elif input_method == 'KNN or k-Nearest Neighbors':\n",
    "            model = KNeighborsClassifier(n_neighbors = 4)\n",
    "\n",
    "        elif input_method == 'Gaussian Naive Bayes':\n",
    "            model = GaussianNB()\n",
    "\n",
    "        elif input_method == 'Perceptron':\n",
    "            model = Perceptron()\n",
    "\n",
    "        elif input_method == 'Linear SVC':\n",
    "            model = LinearSVC(max_iter=1000000)\n",
    "\n",
    "        elif input_method == 'Stochastic Gradient Descent':\n",
    "            model = SGDClassifier()\n",
    "\n",
    "        elif input_method == 'Decision Tree':\n",
    "            model = DecisionTreeClassifier()\n",
    "\n",
    "        elif input_method == 'Random Forest':\n",
    "            model = RandomForestClassifier(n_estimators=20)\n",
    "            \n",
    "        elif input_method == 'Ensemble':\n",
    "            model  = VotingClassifier(ensemble_models, voting='soft') #, weights=[0]*len(ensemble_models)\n",
    "    \n",
    "        # Train\n",
    "        model.fit(X_train_over, Y_train_over)\n",
    "        acc_log = round(model.score(X_train_over, Y_train_over) * 100, 2)\n",
    "        print('trained-acc: ', acc_log)\n",
    "        result_train.append(acc_log)\n",
    "\n",
    "\n",
    "        # Test\n",
    "        print('test- ')\n",
    "        \n",
    "        # f1 score\n",
    "        Y_pred = model.predict(X_test)\n",
    "        acc, pre, rec = get_clf_eval(Y_test, Y_pred)\n",
    "        result_acc.append(acc)\n",
    "        result_pre.append(pre)\n",
    "        result_rec.append(rec)\n",
    "        f1 = f1_score(Y_test, Y_pred, average='weighted')\n",
    "        print('f1 score:{}'.format(f1))\n",
    "        result_f1.append(f1)\n",
    "        print()\n",
    "        print()\n",
    "        \n",
    "        \n",
    "    result_train_df.loc[result_train_df.shape[0]] = result_train\n",
    "    #     result_test_df.loc[result_test_df.shape[0]] = result_test\n",
    "    result_acc_test_df.loc[result_acc_test_df.shape[0]] = result_acc\n",
    "    result_pre_test_df.loc[result_pre_test_df.shape[0]] = result_pre\n",
    "    result_rec_test_df.loc[result_rec_test_df.shape[0]] = result_rec\n",
    "    result_f1score_test_df.loc[result_f1score_test_df.shape[0]] = result_f1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "western-instruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Method:  LogisticRegression\n",
      "trained-acc:  96.54\n",
      "test- \n",
      "accuracy:0.51\n",
      "precision:0.5294708593114665\n",
      "recall:0.51\n",
      "f1 score:0.5181428571428572\n",
      "\n",
      "\n",
      "*** Method:  Support Vector Machines\n",
      "trained-acc:  54.45\n",
      "test- \n",
      "accuracy:0.435\n",
      "precision:0.5217662337662338\n",
      "recall:0.435\n",
      "f1 score:0.4681746544434583\n",
      "\n",
      "\n",
      "*** Method:  KNN or k-Nearest Neighbors\n",
      "trained-acc:  54.94\n",
      "test- \n",
      "accuracy:0.095\n",
      "precision:0.5231521739130435\n",
      "recall:0.095\n",
      "f1 score:0.1120289303840348\n",
      "\n",
      "\n",
      "*** Method:  Gaussian Naive Bayes\n",
      "trained-acc:  72.46\n",
      "test- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.52\n",
      "precision:0.4799\n",
      "recall:0.52\n",
      "f1 score:0.49878009717961935\n",
      "\n",
      "\n",
      "*** Method:  Perceptron\n",
      "trained-acc:  64.55\n",
      "test- \n",
      "accuracy:0.375\n",
      "precision:0.4534753193331891\n",
      "recall:0.375\n",
      "f1 score:0.3054623425355133\n",
      "\n",
      "\n",
      "*** Method:  Linear SVC\n",
      "trained-acc:  95.62\n",
      "test- \n",
      "accuracy:0.48\n",
      "precision:0.5126807048400756\n",
      "recall:0.48\n",
      "f1 score:0.4944544241885282\n",
      "\n",
      "\n",
      "*** Method:  Stochastic Gradient Descent\n",
      "trained-acc:  55.23\n",
      "test- \n",
      "accuracy:0.29\n",
      "precision:0.5979305623872766\n",
      "recall:0.29\n",
      "f1 score:0.37500133863007107\n",
      "\n",
      "\n",
      "*** Method:  Decision Tree\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.375\n",
      "precision:0.477758346651526\n",
      "recall:0.375\n",
      "f1 score:0.4128120687619483\n",
      "\n",
      "\n",
      "*** Method:  Random Forest\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.485\n",
      "precision:0.4911000303735952\n",
      "recall:0.485\n",
      "f1 score:0.4849010989010989\n",
      "\n",
      "\n",
      "*** Method:  Ensemble\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.54\n",
      "precision:0.5359611223799865\n",
      "recall:0.54\n",
      "f1 score:0.5347161051508877\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SMOTE\n",
    "\n",
    "'''\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "preceding-marketing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 적용 전 학습용 피처/레이블 데이터 세트:  (800, 311) (800,)\n",
      "SMOTE 적용 전 레이블 값 분포: \n",
      "3    354\n",
      "1    353\n",
      "2     55\n",
      "0     38\n",
      "Name: newlabel, dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SMOTE 적용 후 학습용 피처/레이블 데이터 세트:  (1110, 311) (1110,)\n",
      "SMOTE 적용 후 레이블 값 분포: \n",
      "3    354\n",
      "1    353\n",
      "0    348\n",
      "2     55\n",
      "Name: newlabel, dtype: int64\n",
      "\n",
      "*** Method:  LogisticRegression\n",
      "trained-acc:  96.4\n",
      "test- \n",
      "accuracy:0.52\n",
      "precision:0.5210637689729942\n",
      "recall:0.52\n",
      "f1 score:0.519519459035588\n",
      "\n",
      "\n",
      "*** Method:  Support Vector Machines\n",
      "trained-acc:  55.95\n",
      "test- \n",
      "accuracy:0.52\n",
      "precision:0.5516637448453094\n",
      "recall:0.52\n",
      "f1 score:0.5316830530488091\n",
      "\n",
      "\n",
      "*** Method:  KNN or k-Nearest Neighbors\n",
      "trained-acc:  45.68\n",
      "test- \n",
      "accuracy:0.145\n",
      "precision:0.38763553113553106\n",
      "recall:0.145\n",
      "f1 score:0.16618448847364511\n",
      "\n",
      "\n",
      "*** Method:  Gaussian Naive Bayes\n",
      "trained-acc:  73.51\n",
      "test- \n",
      "accuracy:0.415\n",
      "precision:0.4695440900562851\n",
      "recall:0.415\n",
      "f1 score:0.43617792642140474\n",
      "\n",
      "\n",
      "*** Method:  Perceptron\n",
      "trained-acc:  68.29\n",
      "test- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.465\n",
      "precision:0.4755957622454595\n",
      "recall:0.465\n",
      "f1 score:0.4030267379679144\n",
      "\n",
      "\n",
      "*** Method:  Linear SVC\n",
      "trained-acc:  94.77\n",
      "test- \n",
      "accuracy:0.46\n",
      "precision:0.482745898257533\n",
      "recall:0.46\n",
      "f1 score:0.4700739589221783\n",
      "\n",
      "\n",
      "*** Method:  Stochastic Gradient Descent\n",
      "trained-acc:  80.54\n",
      "test- \n",
      "accuracy:0.495\n",
      "precision:0.5728351435745329\n",
      "recall:0.495\n",
      "f1 score:0.48409373615793766\n",
      "\n",
      "\n",
      "*** Method:  Decision Tree\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.415\n",
      "precision:0.4438563793462869\n",
      "recall:0.415\n",
      "f1 score:0.42745890726946\n",
      "\n",
      "\n",
      "*** Method:  Random Forest\n",
      "trained-acc:  99.82\n",
      "test- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.515\n",
      "precision:0.5074672897196261\n",
      "recall:0.515\n",
      "f1 score:0.5020489849750008\n",
      "\n",
      "\n",
      "*** Method:  Ensemble\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.555\n",
      "precision:0.5353023504273504\n",
      "recall:0.555\n",
      "f1 score:0.5418661281742587\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "ADASYN\n",
    "'''\n",
    "\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "ada = ADASYN(random_state=0, sampling_strategy  ='minority' )\n",
    "X_train_over, Y_train_over = ada.fit_resample(X_train, Y_train)\n",
    "\n",
    "# print()\n",
    "# training()\n",
    "\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, Y_train.shape)\n",
    "print('SMOTE 적용 전 레이블 값 분포: ')\n",
    "print(pd.Series(Y_train).value_counts())\n",
    "print('-'*100)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, Y_train_over.shape)\n",
    "print('SMOTE 적용 후 레이블 값 분포: ')\n",
    "print(pd.Series(Y_train_over).value_counts())\n",
    "\n",
    "\n",
    "\n",
    "print()\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "central-garbage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ADASYN in module imblearn.over_sampling._adasyn:\n",
      "\n",
      "class ADASYN(imblearn.over_sampling.base.BaseOverSampler)\n",
      " |  ADASYN(*, sampling_strategy='auto', random_state=None, n_neighbors=5, n_jobs=None)\n",
      " |  \n",
      " |  Oversample using Adaptive Synthetic (ADASYN) algorithm.\n",
      " |  \n",
      " |  This method is similar to SMOTE but it generates different number of\n",
      " |  samples depending on an estimate of the local distribution of the class\n",
      " |  to be oversampled.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <smote_adasyn>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  sampling_strategy : float, str, dict or callable, default='auto'\n",
      " |      Sampling information to resample the data set.\n",
      " |  \n",
      " |      - When ``float``, it corresponds to the desired ratio of the number of\n",
      " |        samples in the minority class over the number of samples in the\n",
      " |        majority class after resampling. Therefore, the ratio is expressed as\n",
      " |        :math:`\\alpha_{os} = N_{rm} / N_{M}` where :math:`N_{rm}` is the\n",
      " |        number of samples in the minority class after resampling and\n",
      " |        :math:`N_{M}` is the number of samples in the majority class.\n",
      " |  \n",
      " |          .. warning::\n",
      " |             ``float`` is only available for **binary** classification. An\n",
      " |             error is raised for multi-class classification.\n",
      " |  \n",
      " |      - When ``str``, specify the class targeted by the resampling. The\n",
      " |        number of samples in the different classes will be equalized.\n",
      " |        Possible choices are:\n",
      " |  \n",
      " |          ``'minority'``: resample only the minority class;\n",
      " |  \n",
      " |          ``'not minority'``: resample all classes but the minority class;\n",
      " |  \n",
      " |          ``'not majority'``: resample all classes but the majority class;\n",
      " |  \n",
      " |          ``'all'``: resample all classes;\n",
      " |  \n",
      " |          ``'auto'``: equivalent to ``'not majority'``.\n",
      " |  \n",
      " |      - When ``dict``, the keys correspond to the targeted classes. The\n",
      " |        values correspond to the desired number of samples for each targeted\n",
      " |        class.\n",
      " |  \n",
      " |      - When callable, function taking ``y`` and returns a ``dict``. The keys\n",
      " |        correspond to the targeted classes. The values correspond to the\n",
      " |        desired number of samples for each class.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Control the randomization of the algorithm.\n",
      " |  \n",
      " |      - If int, ``random_state`` is the seed used by the random number\n",
      " |        generator;\n",
      " |      - If ``RandomState`` instance, random_state is the random number\n",
      " |        generator;\n",
      " |      - If ``None``, the random number generator is the ``RandomState``\n",
      " |        instance used by ``np.random``.\n",
      " |  \n",
      " |  n_neighbors : int or estimator object, default=5\n",
      " |      If ``int``, number of nearest neighbours to used to construct synthetic\n",
      " |      samples.  If object, an estimator that inherits from\n",
      " |      :class:`~sklearn.neighbors.base.KNeighborsMixin` that will be used to\n",
      " |      find the k_neighbors.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used during the cross-validation loop.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See\n",
      " |      `Glossary <https://scikit-learn.org/stable/glossary.html#term-n-jobs>`_\n",
      " |      for more details.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SMOTE : Over-sample using SMOTE.\n",
      " |  \n",
      " |  SMOTENC : Over-sample using SMOTE for continuous and categorical features.\n",
      " |  \n",
      " |  SMOTEN : Over-sample using the SMOTE variant specifically for categorical\n",
      " |      features only.\n",
      " |  \n",
      " |  SVMSMOTE : Over-sample using SVM-SMOTE variant.\n",
      " |  \n",
      " |  BorderlineSMOTE : Over-sample using Borderline-SMOTE variant.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The implementation is based on [1]_.\n",
      " |  \n",
      " |  Supports multi-class resampling. A one-vs.-rest scheme is used.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] He, Haibo, Yang Bai, Edwardo A. Garcia, and Shutao Li. \"ADASYN:\n",
      " |     Adaptive synthetic sampling approach for imbalanced learning,\" In IEEE\n",
      " |     International Joint Conference on Neural Networks (IEEE World Congress\n",
      " |     on Computational Intelligence), pp. 1322-1328, 2008.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> from collections import Counter\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> from imblearn.over_sampling import ADASYN # doctest: +NORMALIZE_WHITESPACE\n",
      " |  >>> X, y = make_classification(n_classes=2, class_sep=2,\n",
      " |  ... weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
      " |  ... n_features=20, n_clusters_per_class=1, n_samples=1000,\n",
      " |  ... random_state=10)\n",
      " |  >>> print('Original dataset shape %s' % Counter(y))\n",
      " |  Original dataset shape Counter({1: 900, 0: 100})\n",
      " |  >>> ada = ADASYN(random_state=42)\n",
      " |  >>> X_res, y_res = ada.fit_resample(X, y)\n",
      " |  >>> print('Resampled dataset shape %s' % Counter(y_res))\n",
      " |  Resampled dataset shape Counter({0: 904, 1: 900})\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ADASYN\n",
      " |      imblearn.over_sampling.base.BaseOverSampler\n",
      " |      imblearn.base.BaseSampler\n",
      " |      imblearn.base.SamplerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, sampling_strategy='auto', random_state=None, n_neighbors=5, n_jobs=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from imblearn.base.SamplerMixin:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Check inputs and statistics of the sampler.\n",
      " |      \n",
      " |      You should use ``fit_resample`` in all cases.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, dataframe, sparse matrix} of shape                 (n_samples, n_features)\n",
      " |          Data array.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Return the instance itself.\n",
      " |  \n",
      " |  fit_resample(self, X, y)\n",
      " |      Resample the dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, dataframe, sparse matrix} of shape                 (n_samples, n_features)\n",
      " |          Matrix containing the data which have to be sampled.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Corresponding label for each sample in X.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_resampled : {array-like, dataframe, sparse matrix} of shape                 (n_samples_new, n_features)\n",
      " |          The array containing the resampled data.\n",
      " |      \n",
      " |      y_resampled : array-like of shape (n_samples_new,)\n",
      " |          The corresponding label of `X_resampled`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ADASYN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "electronic-recommendation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 적용 전 학습용 피처/레이블 데이터 세트:  (800, 311) (800,)\n",
      "SMOTE 적용 전 레이블 값 분포: \n",
      "3    354\n",
      "1    353\n",
      "2     55\n",
      "0     38\n",
      "Name: newlabel, dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SMOTE 적용 후 학습용 피처/레이블 데이터 세트:  (1382, 311) (1382,)\n",
      "SMOTE 적용 후 레이블 값 분포: \n",
      "2    354\n",
      "0    353\n",
      "1    338\n",
      "3    337\n",
      "Name: newlabel, dtype: int64\n",
      "*** Method:  LogisticRegression\n",
      "trained-acc:  97.32\n",
      "test- \n",
      "accuracy:0.495\n",
      "precision:0.5314094614963004\n",
      "recall:0.495\n",
      "f1 score:0.5111169765988202\n",
      "\n",
      "\n",
      "*** Method:  Support Vector Machines\n",
      "trained-acc:  53.91\n",
      "test- \n",
      "accuracy:0.4\n",
      "precision:0.5097702617198735\n",
      "recall:0.4\n",
      "f1 score:0.43969487499156024\n",
      "\n",
      "\n",
      "*** Method:  KNN or k-Nearest Neighbors\n",
      "trained-acc:  54.92\n",
      "test- \n",
      "accuracy:0.085\n",
      "precision:0.5252218826924709\n",
      "recall:0.085\n",
      "f1 score:0.0929704790218915\n",
      "\n",
      "\n",
      "*** Method:  Gaussian Naive Bayes\n",
      "trained-acc:  73.01\n",
      "test- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.5\n",
      "precision:0.4615000000000001\n",
      "recall:0.5\n",
      "f1 score:0.4796275686093543\n",
      "\n",
      "\n",
      "*** Method:  Perceptron\n",
      "trained-acc:  73.59\n",
      "test- \n",
      "accuracy:0.495\n",
      "precision:0.5133188736681887\n",
      "recall:0.495\n",
      "f1 score:0.4563749017711419\n",
      "\n",
      "\n",
      "*** Method:  Linear SVC\n",
      "trained-acc:  96.53\n",
      "test- \n",
      "accuracy:0.455\n",
      "precision:0.4995618474662592\n",
      "recall:0.455\n",
      "f1 score:0.4721306330961504\n",
      "\n",
      "\n",
      "*** Method:  Stochastic Gradient Descent\n",
      "trained-acc:  61.22\n",
      "test- \n",
      "accuracy:0.43\n",
      "precision:0.5306625577812017\n",
      "recall:0.43\n",
      "f1 score:0.4551864144263746\n",
      "\n",
      "\n",
      "*** Method:  Decision Tree\n",
      "trained-acc:  100.0\n",
      "test- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.38\n",
      "precision:0.4974558284852402\n",
      "recall:0.38\n",
      "f1 score:0.42247977285872024\n",
      "\n",
      "\n",
      "*** Method:  Random Forest\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.475\n",
      "precision:0.4698536656041727\n",
      "recall:0.475\n",
      "f1 score:0.46994534770429913\n",
      "\n",
      "\n",
      "*** Method:  Ensemble\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.54\n",
      "precision:0.5304709677419355\n",
      "recall:0.54\n",
      "f1 score:0.531688421561518\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SMOTETomek\n",
    "\n",
    "'''\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "smtom = SMOTETomek(random_state=139)\n",
    "X_train_over, Y_train_over = smtom.fit_resample(X_train, Y_train)\n",
    "\n",
    "# print()\n",
    "# training()\n",
    "\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, Y_train.shape)\n",
    "print('SMOTE 적용 전 레이블 값 분포: ')\n",
    "print(pd.Series(Y_train).value_counts())\n",
    "print('-'*100)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, Y_train_over.shape)\n",
    "print('SMOTE 적용 후 레이블 값 분포: ')\n",
    "print(pd.Series(Y_train_over).value_counts())\n",
    "\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "prepared-development",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SMOTE 적용 전 학습용 피처/레이블 데이터 세트:  (800, 311) (800,)\n",
      "SMOTE 적용 전 레이블 값 분포: \n",
      "3    354\n",
      "1    353\n",
      "2     55\n",
      "0     38\n",
      "Name: newlabel, dtype: int64\n",
      "----------------------------------------------------------------------------------------------------\n",
      "SMOTE 적용 후 학습용 피처/레이블 데이터 세트:  (705, 311) (705,)\n",
      "SMOTE 적용 후 레이블 값 분포: \n",
      "2    349\n",
      "0    342\n",
      "3      9\n",
      "1      5\n",
      "Name: newlabel, dtype: int64\n",
      "*** Method:  LogisticRegression\n",
      "trained-acc:  98.72\n",
      "test- \n",
      "accuracy:0.07\n",
      "precision:0.6208869636963696\n",
      "recall:0.07\n",
      "f1 score:0.06386613585142997\n",
      "\n",
      "\n",
      "*** Method:  Support Vector Machines\n",
      "trained-acc:  77.87\n",
      "test- \n",
      "accuracy:0.025\n",
      "precision:0.002106125122830003\n",
      "recall:0.025\n",
      "f1 score:0.003802469135802469\n",
      "\n",
      "\n",
      "*** Method:  KNN or k-Nearest Neighbors\n",
      "trained-acc:  97.59\n",
      "test- \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:0.04\n",
      "precision:0.0032930756843800317\n",
      "recall:0.04\n",
      "f1 score:0.006074022829470771\n",
      "\n",
      "\n",
      "*** Method:  Gaussian Naive Bayes\n",
      "trained-acc:  54.04\n",
      "test- \n",
      "accuracy:0.03\n",
      "precision:0.000967741935483871\n",
      "recall:0.03\n",
      "f1 score:0.001875\n",
      "\n",
      "\n",
      "*** Method:  Perceptron\n",
      "trained-acc:  80.71\n",
      "test- \n",
      "accuracy:0.025\n",
      "precision:0.002019756200084069\n",
      "recall:0.025\n",
      "f1 score:0.003678977272727273\n",
      "\n",
      "\n",
      "*** Method:  Linear SVC\n",
      "trained-acc:  99.72\n",
      "test- \n",
      "accuracy:0.085\n",
      "precision:0.6766813067830278\n",
      "recall:0.085\n",
      "f1 score:0.08266247641247641\n",
      "\n",
      "\n",
      "*** Method:  Stochastic Gradient Descent\n",
      "trained-acc:  80.85\n",
      "test- \n",
      "accuracy:0.04\n",
      "precision:0.0029417879417879423\n",
      "recall:0.04\n",
      "f1 score:0.0054648625054561336\n",
      "\n",
      "\n",
      "*** Method:  Decision Tree\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.085\n",
      "precision:0.6158145917823932\n",
      "recall:0.085\n",
      "f1 score:0.11375236432540509\n",
      "\n",
      "\n",
      "*** Method:  Random Forest\n",
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.025\n",
      "precision:0.002235576923076923\n",
      "recall:0.025\n",
      "f1 score:0.004097007223942209\n",
      "\n",
      "\n",
      "*** Method:  Ensemble\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trained-acc:  100.0\n",
      "test- \n",
      "accuracy:0.025\n",
      "precision:0.0023200494284831637\n",
      "recall:0.025\n",
      "f1 score:0.004201416207710465\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\enssel\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1245: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "SMOTEENN\n",
    "\n",
    "'''\n",
    "\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "smenn = SMOTEENN(random_state=139, sampling_strategy  ='all')\n",
    "X_train_over, Y_train_over = smenn.fit_resample(X_train, Y_train)\n",
    "\n",
    "# print()\n",
    "# training()\n",
    "\n",
    "print('SMOTE 적용 전 학습용 피처/레이블 데이터 세트: ', X_train.shape, Y_train.shape)\n",
    "print('SMOTE 적용 전 레이블 값 분포: ')\n",
    "print(pd.Series(Y_train).value_counts())\n",
    "print('-'*100)\n",
    "print('SMOTE 적용 후 학습용 피처/레이블 데이터 세트: ', X_train_over.shape, Y_train_over.shape)\n",
    "print('SMOTE 적용 후 레이블 값 분포: ')\n",
    "print(pd.Series(Y_train_over).value_counts())\n",
    "\n",
    "training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "canadian-roulette",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class SMOTEENN in module imblearn.combine._smote_enn:\n",
      "\n",
      "class SMOTEENN(imblearn.base.BaseSampler)\n",
      " |  SMOTEENN(*, sampling_strategy='auto', random_state=None, smote=None, enn=None, n_jobs=None)\n",
      " |  \n",
      " |  Over-sampling using SMOTE and cleaning using ENN.\n",
      " |  \n",
      " |  Combine over- and under-sampling using SMOTE and Edited Nearest Neighbours.\n",
      " |  \n",
      " |  Read more in the :ref:`User Guide <combine>`.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  sampling_strategy : float, str, dict or callable, default='auto'\n",
      " |      Sampling information to resample the data set.\n",
      " |  \n",
      " |      - When ``float``, it corresponds to the desired ratio of the number of\n",
      " |        samples in the minority class over the number of samples in the\n",
      " |        majority class after resampling. Therefore, the ratio is expressed as\n",
      " |        :math:`\\alpha_{os} = N_{rm} / N_{M}` where :math:`N_{rm}` is the\n",
      " |        number of samples in the minority class after resampling and\n",
      " |        :math:`N_{M}` is the number of samples in the majority class.\n",
      " |  \n",
      " |          .. warning::\n",
      " |             ``float`` is only available for **binary** classification. An\n",
      " |             error is raised for multi-class classification.\n",
      " |  \n",
      " |      - When ``str``, specify the class targeted by the resampling. The\n",
      " |        number of samples in the different classes will be equalized.\n",
      " |        Possible choices are:\n",
      " |  \n",
      " |          ``'minority'``: resample only the minority class;\n",
      " |  \n",
      " |          ``'not minority'``: resample all classes but the minority class;\n",
      " |  \n",
      " |          ``'not majority'``: resample all classes but the majority class;\n",
      " |  \n",
      " |          ``'all'``: resample all classes;\n",
      " |  \n",
      " |          ``'auto'``: equivalent to ``'not majority'``.\n",
      " |  \n",
      " |      - When ``dict``, the keys correspond to the targeted classes. The\n",
      " |        values correspond to the desired number of samples for each targeted\n",
      " |        class.\n",
      " |  \n",
      " |      - When callable, function taking ``y`` and returns a ``dict``. The keys\n",
      " |        correspond to the targeted classes. The values correspond to the\n",
      " |        desired number of samples for each class.\n",
      " |  \n",
      " |  random_state : int, RandomState instance, default=None\n",
      " |      Control the randomization of the algorithm.\n",
      " |  \n",
      " |      - If int, ``random_state`` is the seed used by the random number\n",
      " |        generator;\n",
      " |      - If ``RandomState`` instance, random_state is the random number\n",
      " |        generator;\n",
      " |      - If ``None``, the random number generator is the ``RandomState``\n",
      " |        instance used by ``np.random``.\n",
      " |  \n",
      " |  smote : sampler object, default=None\n",
      " |      The :class:`~imblearn.over_sampling.SMOTE` object to use. If not given,\n",
      " |      a :class:`~imblearn.over_sampling.SMOTE` object with default parameters\n",
      " |      will be given.\n",
      " |  \n",
      " |  enn : sampler object, default=None\n",
      " |      The :class:`~imblearn.under_sampling.EditedNearestNeighbours` object\n",
      " |      to use. If not given, a\n",
      " |      :class:`~imblearn.under_sampling.EditedNearestNeighbours` object with\n",
      " |      sampling strategy='all' will be given.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      Number of CPU cores used during the cross-validation loop.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See\n",
      " |      `Glossary <https://scikit-learn.org/stable/glossary.html#term-n-jobs>`_\n",
      " |      for more details.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  SMOTETomek : Over-sample using SMOTE followed by under-sampling removing\n",
      " |      the Tomek's links.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  The method is presented in [1]_.\n",
      " |  \n",
      " |  Supports multi-class resampling. Refer to SMOTE and ENN regarding the\n",
      " |  scheme which used.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  .. [1] G. Batista, R. C. Prati, M. C. Monard. \"A study of the behavior of\n",
      " |     several methods for balancing machine learning training data,\" ACM\n",
      " |     Sigkdd Explorations Newsletter 6 (1), 20-29, 2004.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  \n",
      " |  >>> from collections import Counter\n",
      " |  >>> from sklearn.datasets import make_classification\n",
      " |  >>> from imblearn.combine import SMOTEENN # doctest: +NORMALIZE_WHITESPACE\n",
      " |  >>> X, y = make_classification(n_classes=2, class_sep=2,\n",
      " |  ... weights=[0.1, 0.9], n_informative=3, n_redundant=1, flip_y=0,\n",
      " |  ... n_features=20, n_clusters_per_class=1, n_samples=1000, random_state=10)\n",
      " |  >>> print('Original dataset shape %s' % Counter(y))\n",
      " |  Original dataset shape Counter({1: 900, 0: 100})\n",
      " |  >>> sme = SMOTEENN(random_state=42)\n",
      " |  >>> X_res, y_res = sme.fit_resample(X, y)\n",
      " |  >>> print('Resampled dataset shape %s' % Counter(y_res))\n",
      " |  Resampled dataset shape Counter({0: 900, 1: 881})\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      SMOTEENN\n",
      " |      imblearn.base.BaseSampler\n",
      " |      imblearn.base.SamplerMixin\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, sampling_strategy='auto', random_state=None, smote=None, enn=None, n_jobs=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from imblearn.base.SamplerMixin:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Check inputs and statistics of the sampler.\n",
      " |      \n",
      " |      You should use ``fit_resample`` in all cases.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, dataframe, sparse matrix} of shape                 (n_samples, n_features)\n",
      " |          Data array.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Target array.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Return the instance itself.\n",
      " |  \n",
      " |  fit_resample(self, X, y)\n",
      " |      Resample the dataset.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, dataframe, sparse matrix} of shape                 (n_samples, n_features)\n",
      " |          Matrix containing the data which have to be sampled.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,)\n",
      " |          Corresponding label for each sample in X.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      X_resampled : {array-like, dataframe, sparse matrix} of shape                 (n_samples_new, n_features)\n",
      " |          The array containing the resampled data.\n",
      " |      \n",
      " |      y_resampled : array-like of shape (n_samples_new,)\n",
      " |          The corresponding label of `X_resampled`.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : dict\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
      " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
      " |      possible to update each component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : estimator instance\n",
      " |          Estimator instance.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(SMOTEENN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "contemporary-motel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** train result ***\n",
      "\n",
      "                      LogisticRegression  Support Vector Machines  \\\n",
      "Basic                              85.62                    70.00   \n",
      "Time_outlier                       95.25                    56.25   \n",
      "Time_/365                          86.50                    70.00   \n",
      "Clinic_outlier                     95.12                    56.25   \n",
      "Clinic_0-4(2steps)                 93.50                    54.37   \n",
      "Clinic_0-1(resizing)               92.88                    53.12   \n",
      "SMOTE                              96.54                    54.45   \n",
      "ADASYN                             96.40                    55.95   \n",
      "SMOTETomek                         97.32                    53.91   \n",
      "SMOTEENN                           98.72                    77.87   \n",
      "\n",
      "                      KNN or k-Nearest Neighbors  Gaussian Naive Bayes  \\\n",
      "Basic                                      65.38                 38.62   \n",
      "Time_outlier                               70.50                 48.25   \n",
      "Time_/365                                  65.75                 39.12   \n",
      "Clinic_outlier                             70.38                 48.00   \n",
      "Clinic_0-4(2steps)                         68.25                 47.75   \n",
      "Clinic_0-1(resizing)                       65.62                 48.00   \n",
      "SMOTE                                      54.94                 72.46   \n",
      "ADASYN                                     45.68                 73.51   \n",
      "SMOTETomek                                 54.92                 73.01   \n",
      "SMOTEENN                                   97.59                 54.04   \n",
      "\n",
      "                      Perceptron  Linear SVC  Stochastic Gradient Descent  \\\n",
      "Basic                      64.88       86.00                        81.12   \n",
      "Time_outlier               69.12       94.62                        77.75   \n",
      "Time_/365                  77.12       89.62                        78.12   \n",
      "Clinic_outlier             52.50       94.62                        76.25   \n",
      "Clinic_0-4(2steps)         66.38       92.88                        75.88   \n",
      "Clinic_0-1(resizing)        8.88       94.12                        50.38   \n",
      "SMOTE                      64.55       95.62                        55.23   \n",
      "ADASYN                     68.29       94.77                        80.54   \n",
      "SMOTETomek                 73.59       96.53                        61.22   \n",
      "SMOTEENN                   80.71       99.72                        80.85   \n",
      "\n",
      "                      Decision Tree  Random Forest  Ensemble  \n",
      "Basic                         100.0          99.88     100.0  \n",
      "Time_outlier                  100.0          99.88     100.0  \n",
      "Time_/365                     100.0         100.00     100.0  \n",
      "Clinic_outlier                100.0          99.75     100.0  \n",
      "Clinic_0-4(2steps)            100.0         100.00     100.0  \n",
      "Clinic_0-1(resizing)          100.0          99.75     100.0  \n",
      "SMOTE                         100.0         100.00     100.0  \n",
      "ADASYN                        100.0          99.82     100.0  \n",
      "SMOTETomek                    100.0         100.00     100.0  \n",
      "SMOTEENN                      100.0         100.00     100.0  \n",
      "\n",
      "\n",
      "\n",
      "*** test acc ***\n",
      "\n",
      "                      LogisticRegression  Support Vector Machines  \\\n",
      "Basic                              0.435                    0.485   \n",
      "Time_outlier                       0.545                    0.590   \n",
      "Time_/365                          0.445                    0.485   \n",
      "Clinic_outlier                     0.545                    0.590   \n",
      "Clinic_0-4(2steps)                 0.515                    0.580   \n",
      "Clinic_0-1(resizing)               0.530                    0.585   \n",
      "SMOTE                              0.510                    0.435   \n",
      "ADASYN                             0.520                    0.520   \n",
      "SMOTETomek                         0.495                    0.400   \n",
      "SMOTEENN                           0.070                    0.025   \n",
      "\n",
      "                      KNN or k-Nearest Neighbors  Gaussian Naive Bayes  \\\n",
      "Basic                                      0.495                 0.285   \n",
      "Time_outlier                               0.515                 0.330   \n",
      "Time_/365                                  0.500                 0.315   \n",
      "Clinic_outlier                             0.520                 0.330   \n",
      "Clinic_0-4(2steps)                         0.525                 0.320   \n",
      "Clinic_0-1(resizing)                       0.500                 0.330   \n",
      "SMOTE                                      0.095                 0.520   \n",
      "ADASYN                                     0.145                 0.415   \n",
      "SMOTETomek                                 0.085                 0.500   \n",
      "SMOTEENN                                   0.040                 0.030   \n",
      "\n",
      "                      Perceptron  Linear SVC  Stochastic Gradient Descent  \\\n",
      "Basic                      0.450       0.425                        0.440   \n",
      "Time_outlier               0.580       0.495                        0.560   \n",
      "Time_/365                  0.390       0.460                        0.390   \n",
      "Clinic_outlier             0.530       0.500                        0.570   \n",
      "Clinic_0-4(2steps)         0.520       0.465                        0.540   \n",
      "Clinic_0-1(resizing)       0.035       0.495                        0.385   \n",
      "SMOTE                      0.375       0.480                        0.290   \n",
      "ADASYN                     0.465       0.460                        0.495   \n",
      "SMOTETomek                 0.495       0.455                        0.430   \n",
      "SMOTEENN                   0.025       0.085                        0.040   \n",
      "\n",
      "                      Decision Tree  Random Forest  Ensemble  \n",
      "Basic                         0.390          0.420     0.505  \n",
      "Time_outlier                  0.420          0.520     0.560  \n",
      "Time_/365                     0.415          0.480     0.560  \n",
      "Clinic_outlier                0.455          0.490     0.545  \n",
      "Clinic_0-4(2steps)            0.475          0.480     0.555  \n",
      "Clinic_0-1(resizing)          0.410          0.530     0.560  \n",
      "SMOTE                         0.375          0.485     0.540  \n",
      "ADASYN                        0.415          0.515     0.555  \n",
      "SMOTETomek                    0.380          0.475     0.540  \n",
      "SMOTEENN                      0.085          0.025     0.025  \n",
      "\n",
      "\n",
      "\n",
      "*** test pre ***\n",
      "\n",
      "                      LogisticRegression  Support Vector Machines  \\\n",
      "Basic                           0.438923                 0.445895   \n",
      "Time_outlier                    0.532783                 0.546930   \n",
      "Time_/365                       0.446759                 0.445895   \n",
      "Clinic_outlier                  0.532783                 0.546930   \n",
      "Clinic_0-4(2steps)              0.508563                 0.537908   \n",
      "Clinic_0-1(resizing)            0.529206                 0.544245   \n",
      "SMOTE                           0.529471                 0.521766   \n",
      "ADASYN                          0.521064                 0.551664   \n",
      "SMOTETomek                      0.531409                 0.509770   \n",
      "SMOTEENN                        0.620887                 0.002106   \n",
      "\n",
      "                      KNN or k-Nearest Neighbors  Gaussian Naive Bayes  \\\n",
      "Basic                                   0.490044              0.484237   \n",
      "Time_outlier                            0.513028              0.514334   \n",
      "Time_/365                               0.496914              0.532586   \n",
      "Clinic_outlier                          0.517721              0.514334   \n",
      "Clinic_0-4(2steps)                      0.495247              0.506494   \n",
      "Clinic_0-1(resizing)                    0.491047              0.514334   \n",
      "SMOTE                                   0.523152              0.479900   \n",
      "ADASYN                                  0.387636              0.469544   \n",
      "SMOTETomek                              0.525222              0.461500   \n",
      "SMOTEENN                                0.003293              0.000968   \n",
      "\n",
      "                      Perceptron  Linear SVC  Stochastic Gradient Descent  \\\n",
      "Basic                   0.421160    0.435927                     0.443189   \n",
      "Time_outlier            0.561897    0.498129                     0.563009   \n",
      "Time_/365               0.433871    0.475007                     0.434035   \n",
      "Clinic_outlier          0.522642    0.506359                     0.544887   \n",
      "Clinic_0-4(2steps)      0.505778    0.477752                     0.544735   \n",
      "Clinic_0-1(resizing)    0.324095    0.500043                     0.561710   \n",
      "SMOTE                   0.453475    0.512681                     0.597931   \n",
      "ADASYN                  0.475596    0.482746                     0.572835   \n",
      "SMOTETomek              0.513319    0.499562                     0.530663   \n",
      "SMOTEENN                0.002020    0.676681                     0.002942   \n",
      "\n",
      "                      Decision Tree  Random Forest  Ensemble  \n",
      "Basic                      0.416190       0.388630  0.465633  \n",
      "Time_outlier               0.445833       0.483327  0.519559  \n",
      "Time_/365                  0.427384       0.444464  0.520743  \n",
      "Clinic_outlier             0.466548       0.452195  0.528742  \n",
      "Clinic_0-4(2steps)         0.477288       0.443559  0.517807  \n",
      "Clinic_0-1(resizing)       0.432797       0.491350  0.542756  \n",
      "SMOTE                      0.477758       0.491100  0.535961  \n",
      "ADASYN                     0.443856       0.507467  0.535302  \n",
      "SMOTETomek                 0.497456       0.469854  0.530471  \n",
      "SMOTEENN                   0.615815       0.002236  0.002320  \n",
      "\n",
      "\n",
      "\n",
      "*** test rec ***\n",
      "\n",
      "                      LogisticRegression  Support Vector Machines  \\\n",
      "Basic                              0.435                    0.485   \n",
      "Time_outlier                       0.545                    0.590   \n",
      "Time_/365                          0.445                    0.485   \n",
      "Clinic_outlier                     0.545                    0.590   \n",
      "Clinic_0-4(2steps)                 0.515                    0.580   \n",
      "Clinic_0-1(resizing)               0.530                    0.585   \n",
      "SMOTE                              0.510                    0.435   \n",
      "ADASYN                             0.520                    0.520   \n",
      "SMOTETomek                         0.495                    0.400   \n",
      "SMOTEENN                           0.070                    0.025   \n",
      "\n",
      "                      KNN or k-Nearest Neighbors  Gaussian Naive Bayes  \\\n",
      "Basic                                      0.495                 0.285   \n",
      "Time_outlier                               0.515                 0.330   \n",
      "Time_/365                                  0.500                 0.315   \n",
      "Clinic_outlier                             0.520                 0.330   \n",
      "Clinic_0-4(2steps)                         0.525                 0.320   \n",
      "Clinic_0-1(resizing)                       0.500                 0.330   \n",
      "SMOTE                                      0.095                 0.520   \n",
      "ADASYN                                     0.145                 0.415   \n",
      "SMOTETomek                                 0.085                 0.500   \n",
      "SMOTEENN                                   0.040                 0.030   \n",
      "\n",
      "                      Perceptron  Linear SVC  Stochastic Gradient Descent  \\\n",
      "Basic                      0.450       0.425                        0.440   \n",
      "Time_outlier               0.580       0.495                        0.560   \n",
      "Time_/365                  0.390       0.460                        0.390   \n",
      "Clinic_outlier             0.530       0.500                        0.570   \n",
      "Clinic_0-4(2steps)         0.520       0.465                        0.540   \n",
      "Clinic_0-1(resizing)       0.035       0.495                        0.385   \n",
      "SMOTE                      0.375       0.480                        0.290   \n",
      "ADASYN                     0.465       0.460                        0.495   \n",
      "SMOTETomek                 0.495       0.455                        0.430   \n",
      "SMOTEENN                   0.025       0.085                        0.040   \n",
      "\n",
      "                      Decision Tree  Random Forest  Ensemble  \n",
      "Basic                         0.390          0.420     0.505  \n",
      "Time_outlier                  0.420          0.520     0.560  \n",
      "Time_/365                     0.415          0.480     0.560  \n",
      "Clinic_outlier                0.455          0.490     0.545  \n",
      "Clinic_0-4(2steps)            0.475          0.480     0.555  \n",
      "Clinic_0-1(resizing)          0.410          0.530     0.560  \n",
      "SMOTE                         0.375          0.485     0.540  \n",
      "ADASYN                        0.415          0.515     0.555  \n",
      "SMOTETomek                    0.380          0.475     0.540  \n",
      "SMOTEENN                      0.085          0.025     0.025  \n",
      "\n",
      "\n",
      "\n",
      "*** test f1 ***\n",
      "\n",
      "                      LogisticRegression  Support Vector Machines  \\\n",
      "Basic                           0.435505                 0.464598   \n",
      "Time_outlier                    0.538417                 0.560804   \n",
      "Time_/365                       0.444322                 0.464598   \n",
      "Clinic_outlier                  0.538417                 0.560804   \n",
      "Clinic_0-4(2steps)              0.511617                 0.550091   \n",
      "Clinic_0-1(resizing)            0.528706                 0.553428   \n",
      "SMOTE                           0.518143                 0.468175   \n",
      "ADASYN                          0.519519                 0.531683   \n",
      "SMOTETomek                      0.511117                 0.439695   \n",
      "SMOTEENN                        0.063866                 0.003802   \n",
      "\n",
      "                      KNN or k-Nearest Neighbors  Gaussian Naive Bayes  \\\n",
      "Basic                                   0.484129              0.333680   \n",
      "Time_outlier                            0.496410              0.379833   \n",
      "Time_/365                               0.486035              0.371156   \n",
      "Clinic_outlier                          0.502085              0.379833   \n",
      "Clinic_0-4(2steps)                      0.503383              0.369404   \n",
      "Clinic_0-1(resizing)                    0.481414              0.379833   \n",
      "SMOTE                                   0.112029              0.498780   \n",
      "ADASYN                                  0.166184              0.436178   \n",
      "SMOTETomek                              0.092970              0.479628   \n",
      "SMOTEENN                                0.006074              0.001875   \n",
      "\n",
      "                      Perceptron  Linear SVC  Stochastic Gradient Descent  \\\n",
      "Basic                   0.398234    0.429966                     0.436048   \n",
      "Time_outlier            0.563838    0.496470                     0.560649   \n",
      "Time_/365               0.410718    0.467362                     0.405313   \n",
      "Clinic_outlier          0.440901    0.502779                     0.554287   \n",
      "Clinic_0-4(2steps)      0.491376    0.470891                     0.512773   \n",
      "Clinic_0-1(resizing)    0.020878    0.496961                     0.318908   \n",
      "SMOTE                   0.305462    0.494454                     0.375001   \n",
      "ADASYN                  0.403027    0.470074                     0.484094   \n",
      "SMOTETomek              0.456375    0.472131                     0.455186   \n",
      "SMOTEENN                0.003679    0.082662                     0.005465   \n",
      "\n",
      "                      Decision Tree  Random Forest  Ensemble  \n",
      "Basic                      0.399558       0.402151  0.483851  \n",
      "Time_outlier               0.428313       0.498393  0.538175  \n",
      "Time_/365                  0.419835       0.458039  0.537732  \n",
      "Clinic_outlier             0.459144       0.469062  0.528409  \n",
      "Clinic_0-4(2steps)         0.474372       0.457947  0.531891  \n",
      "Clinic_0-1(resizing)       0.417886       0.504603  0.542299  \n",
      "SMOTE                      0.412812       0.484901  0.534716  \n",
      "ADASYN                     0.427459       0.502049  0.541866  \n",
      "SMOTETomek                 0.422480       0.469945  0.531688  \n",
      "SMOTEENN                   0.113752       0.004097  0.004201  \n",
      "\n",
      "\n",
      "\n",
      "*** train acc describe ***\n",
      "\n",
      "LogisticRegression              98.72\n",
      "Support Vector Machines         77.87\n",
      "KNN or k-Nearest Neighbors      97.59\n",
      "Gaussian Naive Bayes            73.51\n",
      "Perceptron                      80.71\n",
      "Linear SVC                      99.72\n",
      "Stochastic Gradient Descent     81.12\n",
      "Decision Tree                  100.00\n",
      "Random Forest                  100.00\n",
      "Ensemble                       100.00\n",
      "Name: max, dtype: float64\n",
      "*** test acc describe ***\n",
      "\n",
      "LogisticRegression             0.545\n",
      "Support Vector Machines        0.590\n",
      "KNN or k-Nearest Neighbors     0.525\n",
      "Gaussian Naive Bayes           0.520\n",
      "Perceptron                     0.580\n",
      "Linear SVC                     0.500\n",
      "Stochastic Gradient Descent    0.570\n",
      "Decision Tree                  0.475\n",
      "Random Forest                  0.530\n",
      "Ensemble                       0.560\n",
      "Name: max, dtype: float64\n",
      "*** test pre describe ***\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression             0.620887\n",
      "Support Vector Machines        0.551664\n",
      "KNN or k-Nearest Neighbors     0.525222\n",
      "Gaussian Naive Bayes           0.532586\n",
      "Perceptron                     0.561897\n",
      "Linear SVC                     0.676681\n",
      "Stochastic Gradient Descent    0.597931\n",
      "Decision Tree                  0.615815\n",
      "Random Forest                  0.507467\n",
      "Ensemble                       0.542756\n",
      "Name: max, dtype: float64\n",
      "*** test rec describe ***\n",
      "\n",
      "LogisticRegression             0.545\n",
      "Support Vector Machines        0.590\n",
      "KNN or k-Nearest Neighbors     0.525\n",
      "Gaussian Naive Bayes           0.520\n",
      "Perceptron                     0.580\n",
      "Linear SVC                     0.500\n",
      "Stochastic Gradient Descent    0.570\n",
      "Decision Tree                  0.475\n",
      "Random Forest                  0.530\n",
      "Ensemble                       0.560\n",
      "Name: max, dtype: float64\n",
      "*** test f1 describe ***\n",
      "\n",
      "LogisticRegression             0.538417\n",
      "Support Vector Machines        0.560804\n",
      "KNN or k-Nearest Neighbors     0.503383\n",
      "Gaussian Naive Bayes           0.498780\n",
      "Perceptron                     0.563838\n",
      "Linear SVC                     0.502779\n",
      "Stochastic Gradient Descent    0.560649\n",
      "Decision Tree                  0.474372\n",
      "Random Forest                  0.504603\n",
      "Ensemble                       0.542299\n",
      "Name: max, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "index = ['Basic','Time_outlier','Time_/365','Clinic_outlier','Clinic_0-4(2steps)','Clinic_0-1(resizing)',\n",
    "        'SMOTE', 'ADASYN', 'SMOTETomek', 'SMOTEENN']\n",
    "\n",
    "print('*** train result ***', end='\\n\\n')\n",
    "result_train_df.index = index\n",
    "print(result_train_df)\n",
    "print(end='\\n\\n\\n')\n",
    "\n",
    "# print('*** test result ***', end='\\n\\n')\n",
    "# result_test_df.index = index\n",
    "# print(result_test_df)\n",
    "# print(end='\\n\\n\\n')\n",
    "\n",
    "print('*** test acc ***', end='\\n\\n')\n",
    "result_acc_test_df.index = index\n",
    "print(result_acc_test_df)\n",
    "print(end='\\n\\n\\n')\n",
    "\n",
    "print('*** test pre ***', end='\\n\\n')\n",
    "result_pre_test_df.index = index\n",
    "print(result_pre_test_df)\n",
    "print(end='\\n\\n\\n')\n",
    "\n",
    "print('*** test rec ***', end='\\n\\n')\n",
    "result_rec_test_df.index = index\n",
    "print(result_rec_test_df)\n",
    "print(end='\\n\\n\\n')\n",
    "\n",
    "print('*** test f1 ***', end='\\n\\n')\n",
    "result_f1score_test_df.index = index\n",
    "print(result_f1score_test_df)\n",
    "print(end='\\n\\n\\n')\n",
    "\n",
    "print('*** train acc describe ***', end='\\n\\n')\n",
    "print(result_train_df.describe().loc['max'])\n",
    "\n",
    "print('*** test acc describe ***', end='\\n\\n')\n",
    "print(result_acc_test_df.describe().loc['max'])\n",
    "\n",
    "print('*** test pre describe ***', end='\\n\\n')\n",
    "print(result_pre_test_df.describe().loc['max'])\n",
    "\n",
    "print('*** test rec describe ***', end='\\n\\n')\n",
    "print(result_rec_test_df.describe().loc['max'])\n",
    "\n",
    "print('*** test f1 describe ***', end='\\n\\n')\n",
    "print(result_f1score_test_df.describe().loc['max'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
